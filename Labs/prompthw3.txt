Okay great - let's now move onto problem 2:

2 Problem 2: Optimizers from Scratch 2.1 Optimizer Implementaiton(20pts)
In this problem, you will implement the commonly-used optimizers from scratch. Train a CNN for MNIST classification on cross-entropy loss with L1-regularization using the following optimizers. Showing the correctness of your implementation by reporting the classification accuracy of your model after training.
1. Momentum method with parameter β = 0.9 (5 pts)
2. Nesterov’s Accelerated Gradient (NAG) with parameter β = 0.95 (5 pts) 3. RMSprop with parameters β = 0.95, γ = 1 and ε = 10−8 (10 pts)
4. Adam with parameters β1 = 0.9, β2 = 0.999, and ε = 10−8 (10 pts)
Note: You can use the Autograd package from Pytorch to compute the gradient when building your optimizer. However, you are NOT allowed to use any built-in optimizers.
Hint: Kingma et al. stated in [1] an alternative implementation of Adam, which has lower clarity but higher computation efficiency. (read the last paragraph before Section 2.1 for that paper)
2.2 Comparing your optimizer efficiency (15pts)
After implementing your optimizers, set your batch size to {4, 8, 16, 32}. Then, compare the changes in training and validation loss for each optimizer at various chosen learning rates (choose at your discretion) throughout the training process by creating plots for each batch size.


Section 2.1 of the paper:



\section*{2.1 Adam's Update Rule}

An important property of Adam's update rule is its careful choice of stepsizes. Assuming \( \epsilon = 0 \), the effective step taken in parameter space at timestep \( t \) is 
\[
\Delta_t = \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t}}.
\] 
The effective stepsize has two upper bounds: 
\[
|\Delta_t| \leq \alpha \cdot \frac{(1 - \beta_1)}{\sqrt{1 - \hat{\beta}_2}} \quad \text{in the case } (1 - \beta_1) > \sqrt{1 - \hat{\beta}_2},
\]
and 
\[
|\Delta_t| \leq \alpha
\]
otherwise. The first case only happens in the most severe case of sparsity: when a gradient has been zero at all timesteps except at the current timestep. For less sparse cases, the effective stepsize will be smaller. When \( (1 - \beta_1) = \sqrt{1 - \hat{\beta}_2} \), we have that \( \frac{|\hat{m}_t|}{\sqrt{\hat{v}_t}} < 1 \) therefore \( |\Delta_t| < \alpha \). In more common scenarios, we will have that \( \frac{\hat{m}_t}{\sqrt{\hat{v}_t}} \approx \pm 1 \) since \( |\mathbb{E}[g_t]| / \sqrt{\mathbb{E}[g^2_t]} \leq 1 \). The effective magnitude of the steps taken in parameter space at each timestep are approximately bounded by the stepsize setting \( \alpha \), i.e., \( |\Delta_t| \lesssim \alpha \). This can be understood as establishing a \emph{trust region} around the current parameter value, beyond which the current gradient estimate does not provide sufficient information. This typically makes it relatively easy to know the right scale of \( \alpha \) in advance. For many machine learning models, for instance, we often know in advance that good optima are with high probability within some set region in parameter space; it is not uncommon, for example, to have a prior distribution over the parameters. Since \( \alpha \) sets (an upper bound of) the magnitude of steps in parameter space, we can often deduce the right order of magnitude of \( \alpha \) such that optima can be reached from \( \theta_0 \) within some number of iterations. With a slight abuse of terminology, we will call the ratio \( \frac{\hat{m}_t}{\sqrt{\hat{v}_t}} \) the \emph{signal-to-noise ratio (SNR)}. With a smaller SNR the effective stepsize \( \Delta_t \) will be closer to zero. This is a desirable property, since a smaller SNR means that there is greater uncertainty about whether the direction of \( \hat{m}_t \) corresponds to the direction of the true gradient. For example, the SNR value typically becomes closer to 0 towards an optimum, leading to smaller effective steps in parameter space: a form of automatic annealing. The effective stepsize \( \Delta_t \) is also invariant to the scale of the gradients; rescaling the gradients \( g \) with factor \( c \) will scale \( \hat{m}_t \) with a factor \( c \) and \( \hat{v}_t \) with a factor \( c^2 \), which cancel out:
\[
c \cdot \frac{\hat{m}_t}{\sqrt{(c^2 \cdot \hat{v}_t)}} = \frac{\hat{m}_t}{\sqrt{\hat{v}_t}}.
\]

Note it is suggested that you use the MINST dataset from torch and it is also suggested that besides this line "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)" which we will have to make for ourself - you should use as much pytorch as needed