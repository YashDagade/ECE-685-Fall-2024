{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Gradients at Linear(in_features=2, out_features=1, bias=True):\n",
      "grad_input: (tensor([-1.8877]), tensor([[0.3304, 0.5020],\n",
      "        [0.0591, 0.0899],\n",
      "        [0.3607, 0.5480],\n",
      "        [0.1089, 0.1654]]), tensor([[-1.0568],\n",
      "        [-1.1180]]))\n",
      "grad_output: (tensor([[-0.7259],\n",
      "        [-0.1299],\n",
      "        [-0.7926],\n",
      "        [-0.2392]]),)\n",
      "Gradients at Linear(in_features=2, out_features=2, bias=True):\n",
      "grad_input: (tensor([0.8591, 1.3053]), tensor([[ 0.2709, -0.1162],\n",
      "        [ 0.0485, -0.0208],\n",
      "        [ 0.2958, -0.1268],\n",
      "        [ 0.0893, -0.0383]]), tensor([[0.5778, 0.8780],\n",
      "        [0.3560, 0.5409]]))\n",
      "grad_output: (tensor([[0.3304, 0.5020],\n",
      "        [0.0591, 0.0899],\n",
      "        [0.3607, 0.5480],\n",
      "        [0.1089, 0.1654]]),)\n",
      "fc1.weight - Weights: tensor([[ 0.4532,  0.6333],\n",
      "        [ 0.2289, -0.6560]]), Gradient: tensor([[0.5778, 0.3560],\n",
      "        [0.8780, 0.5409]])\n",
      "fc1.bias - Weights: tensor([-0.0214,  0.6890]), Gradient: tensor([0.8591, 1.3053])\n",
      "fc2.weight - Weights: tensor([[-0.4445, -0.6803]]), Gradient: tensor([[-1.0568, -1.1180]])\n",
      "fc2.bias - Weights: tensor([0.1852]), Gradient: tensor([-1.8877])\n",
      "Loss: 1.2292660474777222\n",
      "\n",
      "Epoch 2\n",
      "Gradients at Linear(in_features=2, out_features=1, bias=True):\n",
      "grad_input: (tensor([-1.7857]), tensor([[0.3119, 0.4773],\n",
      "        [0.0475, 0.0727],\n",
      "        [0.3399, 0.5203],\n",
      "        [0.0945, 0.1446]]), tensor([[-0.9717],\n",
      "        [-1.0286]]))\n",
      "grad_output: (tensor([[-0.7016],\n",
      "        [-0.1068],\n",
      "        [-0.7648],\n",
      "        [-0.2126]]),)\n",
      "Gradients at Linear(in_features=2, out_features=2, bias=True):\n",
      "grad_input: (tensor([0.7938, 1.2148]), tensor([[ 0.2506, -0.1156],\n",
      "        [ 0.0381, -0.0176],\n",
      "        [ 0.2731, -0.1260],\n",
      "        [ 0.0759, -0.0350]]), tensor([[0.5375, 0.8226],\n",
      "        [0.3242, 0.4962]]))\n",
      "grad_output: (tensor([[0.3119, 0.4773],\n",
      "        [0.0475, 0.0727],\n",
      "        [0.3399, 0.5203],\n",
      "        [0.0945, 0.1446]]),)\n",
      "fc1.weight - Weights: tensor([[ 0.4478,  0.6301],\n",
      "        [ 0.2207, -0.6610]]), Gradient: tensor([[0.5375, 0.3242],\n",
      "        [0.8226, 0.4962]])\n",
      "fc1.bias - Weights: tensor([-0.0293,  0.6768]), Gradient: tensor([0.7938, 1.2148])\n",
      "fc2.weight - Weights: tensor([[-0.4348, -0.6700]]), Gradient: tensor([[-0.9717, -1.0286]])\n",
      "fc2.bias - Weights: tensor([0.2031]), Gradient: tensor([-1.7857])\n",
      "Loss: 1.1336450576782227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yd211/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple neural network with one hidden layer\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)  # Input to hidden layer\n",
    "        self.fc2 = nn.Linear(2, 1)  # Hidden to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the network\n",
    "model = SimpleNN()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy data: 4 samples, 2 features each\n",
    "inputs = torch.tensor([[0.5, 0.3], [0.2, 0.8], [0.9, 0.4], [0.7, 0.6]], requires_grad=True)\n",
    "targets = torch.tensor([[1.0], [0.0], [1.0], [0.0]])\n",
    "\n",
    "# Hook function to print gradients during backpropagation\n",
    "def print_gradients(module, grad_input, grad_output):\n",
    "    print(f'Gradients at {module}:')\n",
    "    print('grad_input:', grad_input)\n",
    "    print('grad_output:', grad_output)\n",
    "\n",
    "# Register hooks for each layer\n",
    "for layer in model.children():\n",
    "    layer.register_backward_hook(print_gradients)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2):  # Run for 2 epochs\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print updated weights after each step\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f'{name} - Weights: {param.data}, Gradient: {param.grad}')\n",
    "    \n",
    "    print(f'Loss: {loss.item()}\\n')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
