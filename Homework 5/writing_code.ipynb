{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC why not\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "image_size = 28\n",
    "nz = 100  # Size of latent vector z\n",
    "num_epochs = 20\n",
    "lr = 0.0002\n",
    "beta1 = 0.5  # Beta1 hyperparameter for Adam optimizers\n",
    "\n",
    "# Data transformation and loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: (batch_size, 1, 28, 28)\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),  # Output: (batch_size, 64, 14, 14)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Second conv layer\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # Output: (batch_size, 128, 7, 7)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Third conv layer\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # Output: (batch_size, 256, 4, 4)\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Fourth conv layer\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),  # Output: (batch_size, 512, 4, 4)\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Output layer\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),  # Output: (batch_size, 1, 1, 1)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1)\n",
    "\n",
    "# Generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: latent vector z of size (batch_size, nz)\n",
    "            nn.Linear(nz, 256 * 7 * 7),\n",
    "            nn.BatchNorm1d(256 * 7 * 7),\n",
    "            nn.ReLU(True),\n",
    "            # Reshape to (batch_size, 256, 7, 7)\n",
    "            nn.Unflatten(1, (256, 7, 7)),\n",
    "            # First ConvTranspose layer\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # Output: (batch_size, 128, 14, 14)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # Second ConvTranspose layer\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # Output: (batch_size, 64, 28, 28)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # Third ConvTranspose layer\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1),  # Output: (batch_size, 32, 28, 28)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            # Output layer\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1),  # Output: (batch_size, 1, 28, 28)\n",
    "            nn.Tanh()  # Output values in [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "# Initialize models\n",
    "netD = Discriminator().to(device)\n",
    "netG = Generator().to(device)\n",
    "\n",
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netD.apply(weights_init)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, device=device)  # For visualization\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ############################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        noise = torch.randn(b_size, nz, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ############################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # Fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Print training stats\n",
    "        if i % 100 == 0:\n",
    "            print(f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] '\n",
    "                  f'Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} '\n",
    "                  f'D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f}/{D_G_z2:.4f}')\n",
    "\n",
    "# Generate 12 new samples\n",
    "netG.eval()\n",
    "with torch.no_grad():\n",
    "    fixed_noise = torch.randn(12, nz, device=device)\n",
    "    fake_images = netG(fixed_noise).detach().cpu()\n",
    "\n",
    "# Plot the images\n",
    "grid = torchvision.utils.make_grid(fake_images, nrow=4, normalize=True)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis('off')\n",
    "plt.title('Generated Images')\n",
    "plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# Part 1.2: GAN as a Pre-Training Framework\n",
    "# ------------------------------\n",
    "\n",
    "# Create a feature extractor from the discriminator\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, discriminator):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        # Copy layers up to before the last Conv2d layer\n",
    "        self.features = nn.Sequential(*list(discriminator.main.children())[:-2])  # Exclude last Conv2d and Sigmoid\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return x\n",
    "\n",
    "# Instantiate the feature extractor\n",
    "feature_extractor = FeatureExtractor(netD).to(device)\n",
    "\n",
    "# Freeze feature extractor parameters\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Prepare 10% of the training data\n",
    "train_indices = np.arange(len(dataset))\n",
    "np.random.shuffle(train_indices)\n",
    "subset_indices = train_indices[:len(dataset) // 10]\n",
    "train_subset = Subset(dataset, subset_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Prepare test data\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Determine the size of the feature vector\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(1, 1, 28, 28, device=device)\n",
    "    features = feature_extractor(dummy_input)\n",
    "    feature_size = features.shape[1]\n",
    "\n",
    "# Define a linear classifier\n",
    "classifier = nn.Linear(feature_size, 10).to(device)\n",
    "\n",
    "# Loss function and optimizer for classifier\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "optimizer_cls = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Train the classifier\n",
    "num_epochs_cls = 10\n",
    "for epoch in range(num_epochs_cls):\n",
    "    classifier.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = feature_extractor(inputs)\n",
    "        outputs = classifier(features)\n",
    "        loss = criterion_cls(outputs, labels)\n",
    "\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100. * correct / total\n",
    "\n",
    "    # Evaluate on test set\n",
    "    classifier.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            features = feature_extractor(inputs)\n",
    "            outputs = classifier(features)\n",
    "            loss = criterion_cls(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = 100. * correct_test / total_test\n",
    "\n",
    "    print(f'[{epoch + 1}/{num_epochs_cls}] Train Loss: {train_loss:.4f} Train Acc: {train_acc:.2f}% '\n",
    "          f'Test Loss: {test_loss:.4f} Test Acc: {test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0\n",
    "num_epochs = 25\n",
    "k = 5  # Number of Gibbs sampling steps\n",
    "M_values = [16, 64, 256]  # Hidden layer sizes\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load KMNIST dataset\n",
    "train_set = torchvision.datasets.KMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.KMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Define the Gaussian-Bernoulli RBM\n",
    "class RBM(nn.Module):\n",
    "    \"\"\"Gaussian-Bernoulli Restricted Boltzmann Machine.\"\"\"\n",
    "\n",
    "    def __init__(self, visible_units: int, hidden_units: int, k: int):\n",
    "        \"\"\"\n",
    "        Initializes the RBM model.\n",
    "\n",
    "        Args:\n",
    "            visible_units (int): Number of visible units (input dimension).\n",
    "            hidden_units (int): Number of hidden units.\n",
    "            k (int): Number of Gibbs sampling steps.\n",
    "        \"\"\"\n",
    "        super(RBM, self).__init__()\n",
    "        self.visible = visible_units\n",
    "        self.hidden = hidden_units\n",
    "        self.k = k\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W = nn.Parameter(torch.randn(hidden_units, visible_units) * 0.01)  # Weight matrix\n",
    "        self.c = nn.Parameter(torch.zeros(visible_units))  # Visible biases\n",
    "        self.b = nn.Parameter(torch.zeros(hidden_units))  # Hidden biases\n",
    "\n",
    "    def sample_bernoulli(self, probs):\n",
    "        \"\"\"Sample binary values from probabilities.\"\"\"\n",
    "        return torch.bernoulli(probs)\n",
    "\n",
    "    def sample_gaussian(self, mean, std=1.0):\n",
    "        \"\"\"Sample continuous values from Gaussian distribution.\"\"\"\n",
    "        return mean + torch.randn(mean.size()).to(device) * std\n",
    "\n",
    "    def p_h_v(self, v):\n",
    "        \"\"\"\n",
    "        Compute the probability of hidden units given visible units.\n",
    "\n",
    "        Args:\n",
    "            v (torch.Tensor): Visible units.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Probability of hidden units.\n",
    "        \"\"\"\n",
    "        return torch.sigmoid(F.linear(v, self.W, self.b))\n",
    "\n",
    "    def p_v_h(self, h):\n",
    "        \"\"\"\n",
    "        Compute the mean of visible units given hidden units.\n",
    "\n",
    "        Args:\n",
    "            h (torch.Tensor): Hidden units.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Mean of visible units.\n",
    "        \"\"\"\n",
    "        return F.linear(h, self.W, self.c)\n",
    "\n",
    "    def free_energy(self, v):\n",
    "        \"\"\"\n",
    "        Compute the free energy for a batch of visible units.\n",
    "\n",
    "        Args:\n",
    "            v (torch.Tensor): Visible units.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Free energy.\n",
    "        \"\"\"\n",
    "        vbias_term = torch.sum((v - self.c) ** 2, dim=1) / 2\n",
    "        wx_b = F.linear(v, self.W, self.b)\n",
    "        hidden_term = torch.sum(torch.log1p(torch.exp(wx_b)), dim=1)\n",
    "        return vbias_term - hidden_term\n",
    "\n",
    "    def forward(self, v):\n",
    "        \"\"\"\n",
    "        Perform Contrastive Divergence (CD-k) to sample from the model.\n",
    "\n",
    "        Args:\n",
    "            v (torch.Tensor): Initial visible units.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed visible units after k Gibbs steps.\n",
    "            torch.Tensor: Probabilities of hidden units after k Gibbs steps.\n",
    "        \"\"\"\n",
    "        v_neg = v\n",
    "        for _ in range(self.k):\n",
    "            # Sample hidden units\n",
    "            p_h = self.p_h_v(v_neg)\n",
    "            h_sample = self.sample_bernoulli(p_h)\n",
    "\n",
    "            # Sample visible units\n",
    "            v_mean = self.p_v_h(h_sample)\n",
    "            v_neg = self.sample_gaussian(v_mean)\n",
    "\n",
    "        p_h_neg = self.p_h_v(v_neg)\n",
    "        return v_neg, p_h_neg\n",
    "\n",
    "def train_rbm(model, train_loader, optimizer, epoch):\n",
    "    \"\"\"Train the RBM model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.view(data.size(0), -1).to(device)\n",
    "        data = data * 2 - 1  # Rescale from [0,1] to [-1,1]\n",
    "\n",
    "        # Positive phase\n",
    "        p_h_v = model.p_h_v(data)\n",
    "        h_sample = model.sample_bernoulli(p_h_v)\n",
    "\n",
    "        # Negative phase\n",
    "        v_neg, p_h_neg = model(data)\n",
    "\n",
    "        # Compute gradients\n",
    "        positive_grad = torch.matmul(p_h_v.t(), data)\n",
    "        negative_grad = torch.matmul(p_h_neg.t(), v_neg)\n",
    "\n",
    "        # Update parameters\n",
    "        loss = torch.mean(model.free_energy(data)) - torch.mean(model.free_energy(v_neg))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % (len(train_loader) // 2) == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}] Batch [{batch_idx + 1}/{len(train_loader)}] '\n",
    "                  f'Loss: {loss.item():.4f}')\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_loss:.4f}')\n",
    "\n",
    "def test_rbm(model, test_loader, epoch):\n",
    "    \"\"\"Evaluate the RBM model on the test set.\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            data = data * 2 - 1  # Rescale from [0,1] to [-1,1]\n",
    "\n",
    "            v_neg, p_h_neg = model(data)\n",
    "            loss = torch.mean(model.free_energy(data)) - torch.mean(model.free_energy(v_neg))\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    print(f'====> Test set loss: {avg_test_loss:.4f}')\n",
    "    return avg_test_loss\n",
    "\n",
    "def reconstruct(model, data):\n",
    "    \"\"\"Reconstruct visible units from the model.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        v_neg, _ = model(data)\n",
    "    return v_neg\n",
    "\n",
    "def plot_reconstructions(original, reconstructed, epoch, M):\n",
    "    \"\"\"Plot original and reconstructed images.\"\"\"\n",
    "    original = original.view(-1, 1, 28, 28).cpu()\n",
    "    reconstructed = reconstructed.view(-1, 1, 28, 28).cpu()\n",
    "    comparison = torch.cat([original[:8], reconstructed[:8]])\n",
    "    grid = make_grid(comparison, nrow=8, padding=2, normalize=True)\n",
    "\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.title(f'Original and Reconstructed Images (Epoch {epoch}, M={M})')\n",
    "    plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def compute_mse(model, data_loader):\n",
    "    \"\"\"Compute Mean Squared Error between original and reconstructed data.\"\"\"\n",
    "    model.eval()\n",
    "    mse = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            data = data * 2 - 1  # Rescale from [0,1] to [-1,1]\n",
    "            v_neg, _ = model(data)\n",
    "            mse += F.mse_loss(v_neg, data, reduction='sum').item()\n",
    "            count += data.size(0)\n",
    "    mse /= count\n",
    "    return mse\n",
    "\n",
    "# Training and Evaluation Loop for different M values\n",
    "for M in M_values:\n",
    "    print(f'\\n======================== Training RBM with M = {M} ========================\\n')\n",
    "    rbm = RBM(visible_units=28*28, hidden_units=M, k=k).to(device)\n",
    "    optimizer = optim.Adam(rbm.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_rbm(rbm, train_loader, optimizer, epoch)\n",
    "        test_loss = test_rbm(rbm, test_loader, epoch)\n",
    "        train_losses.append(test_loss)  # Using test loss as a placeholder\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        # Reconstruct and visualize\n",
    "        data, _ = next(iter(test_loader))\n",
    "        data = data[:32]\n",
    "        data = data.view(data.size(0), -1).to(device)\n",
    "        data = data * 2 - 1  # Rescale to [-1,1]\n",
    "        reconstructed = reconstruct(rbm, data)\n",
    "        plot_reconstructions(data, reconstructed, epoch, M)\n",
    "\n",
    "    # Compute MSE for train and test sets\n",
    "    train_mse = compute_mse(rbm, train_loader)\n",
    "    test_mse = compute_mse(rbm, test_loader)\n",
    "    print(f'RBM with M={M}: Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
    "\n",
    "    # Plotting the loss curves\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, test_losses, label='Test Loss')\n",
    "    plt.title(f'Loss Curves for RBM with M={M}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Reconstruct and visualize final epoch\n",
    "    data, _ = next(iter(test_loader))\n",
    "    data = data[:32]\n",
    "    data_size = data.size()\n",
    "    data = data.view(data.size(0), -1).to(device)\n",
    "    data = data * 2 - 1  # Rescale to [-1,1]\n",
    "    reconstructed = reconstruct(rbm, data)\n",
    "    plot_reconstructions(data, reconstructed, num_epochs, M)\n",
    "\n",
    "    print(f'Optimizer Learning rate: {optimizer.param_groups[0][\"lr\"]:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Hyperparameters\n",
    "n_in = 784  # 28x28 images\n",
    "n_hid = 400\n",
    "z_dim = 20\n",
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "num_epochs = 25\n",
    "validation_split = 0.2  # 20% for validation\n",
    "\n",
    "# Create directory to save models\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "train_val_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "num_train = int((1 - validation_split) * len(train_val_set))\n",
    "num_val = len(train_val_set) - num_train\n",
    "train_set, val_set = random_split(train_val_set, [num_train, num_val])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Function to convert labels to one-hot encoding\n",
    "def one_hot(labels, num_classes=10):\n",
    "    return F.one_hot(labels, num_classes=num_classes).float()\n",
    "\n",
    "# ---------------------------\n",
    "# 3.1 Vanilla VAE Implementation\n",
    "# ---------------------------\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(n_in, n_hid)\n",
    "        self.fc21 = nn.Linear(n_hid, z_dim)  # For mean\n",
    "        self.fc22 = nn.Linear(n_hid, z_dim)  # For log variance\n",
    "\n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(z_dim, n_hid)\n",
    "        self.fc4 = nn.Linear(n_hid, n_in)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encoder forward pass.\"\"\"\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mu = self.fc21(h)\n",
    "        logvar = self.fc22(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Implements: z = mu + epsilon * std\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Decoder forward pass.\"\"\"\n",
    "        h = F.relu(self.fc3(z))\n",
    "        recon_x = torch.sigmoid(self.fc4(h))\n",
    "        return recon_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "# Initialize VAE model\n",
    "vae = VAE(n_in=n_in, n_hid=n_hid, z_dim=z_dim).to(device)\n",
    "optimizer_vae = Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function: ELBO (Reconstruction loss + KL Divergence)\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')  # Reconstruction loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  # KL divergence\n",
    "    return BCE + KLD\n",
    "\n",
    "# Training function for VAE\n",
    "def train_vae(model, optimizer, train_loader, val_loader, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.view(-1, n_in).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch} Average Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in val_loader:\n",
    "            data = data.view(-1, n_in).to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch} Average Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Testing function for VAE\n",
    "def test_vae(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.view(-1, n_in).to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    print(f'====> Test set loss: {avg_test_loss:.4f}')\n",
    "    return avg_test_loss\n",
    "\n",
    "# Training loop for VAE\n",
    "print(\"Training Vanilla VAE...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_vae(vae, optimizer_vae, train_loader, val_loader, epoch)\n",
    "    test_loss = test_vae(vae, test_loader)\n",
    "\n",
    "# Save the trained VAE model\n",
    "torch.save(vae.state_dict(), 'models/vae_fashion_mnist.pth')\n",
    "print(\"VAE model saved to 'models/vae_fashion_mnist.pth'\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3.2 Conditional VAE (C-VAE) Implementation\n",
    "# ---------------------------\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, z_dim, n_classes=10):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(n_in + n_classes, n_hid)\n",
    "        self.fc21 = nn.Linear(n_hid, z_dim)  # For mean\n",
    "        self.fc22 = nn.Linear(n_hid, z_dim)  # For log variance\n",
    "\n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(z_dim + n_classes, n_hid)\n",
    "        self.fc4 = nn.Linear(n_hid, n_in)\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        \"\"\"Encoder forward pass with conditioning.\"\"\"\n",
    "        x = torch.cat([x, c], dim=1)  # Concatenate input with one-hot class vector\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mu = self.fc21(h)\n",
    "        logvar = self.fc22(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Implements: z = mu + epsilon * std\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):\n",
    "        \"\"\"Decoder forward pass with conditioning.\"\"\"\n",
    "        z = torch.cat([z, c], dim=1)  # Concatenate latent vector with one-hot class vector\n",
    "        h = F.relu(self.fc3(z))\n",
    "        recon_x = torch.sigmoid(self.fc4(h))\n",
    "        return recon_x\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"Forward pass through the network with conditioning.\"\"\"\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z, c)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "# Initialize C-VAE model\n",
    "cvae = CVAE(n_in=n_in, n_hid=n_hid, z_dim=z_dim).to(device)\n",
    "optimizer_cvae = Adam(cvae.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training function for C-VAE\n",
    "def train_cvae(model, optimizer, train_loader, val_loader, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data = data.view(-1, n_in).to(device)\n",
    "        labels = labels.to(device)\n",
    "        c = one_hot(labels, num_classes=10).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data, c)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch} C-VAE Average Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in val_loader:\n",
    "            data = data.view(-1, n_in).to(device)\n",
    "            labels = labels.to(device)\n",
    "            c = one_hot(labels, num_classes=10).to(device)\n",
    "            recon_batch, mu, logvar = model(data, c)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch} C-VAE Average Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Testing function for C-VAE\n",
    "def test_cvae(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.view(-1, n_in).to(device)\n",
    "            labels = labels.to(device)\n",
    "            c = one_hot(labels, num_classes=10).to(device)\n",
    "            recon_batch, mu, logvar = model(data, c)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    print(f'====> C-VAE Test set loss: {avg_test_loss:.4f}')\n",
    "    return avg_test_loss\n",
    "\n",
    "# Training loop for C-VAE\n",
    "print(\"\\nTraining Conditional VAE (C-VAE)...\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_cvae(cvae, optimizer_cvae, train_loader, val_loader, epoch)\n",
    "    test_loss = test_cvae(cvae, test_loader)\n",
    "\n",
    "# Save the trained C-VAE model\n",
    "torch.save(cvae.state_dict(), 'models/cvae_fashion_mnist.pth')\n",
    "print(\"C-VAE model saved to 'models/cvae_fashion_mnist.pth'\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3.3 Manifold Comparison using t-SNE\n",
    "# ---------------------------\n",
    "\n",
    "def extract_mu(model, data_loader, conditional=False):\n",
    "    \"\"\"Extracts the mean vectors from the encoder.\"\"\"\n",
    "    model.eval()\n",
    "    mus = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            data = data.view(-1, n_in).to(device)\n",
    "            labels = labels.to(device)\n",
    "            if conditional:\n",
    "                c = one_hot(labels, num_classes=10).to(device)\n",
    "                mu, _ = model.encode(data, c)\n",
    "            else:\n",
    "                mu, _ = model.encode(data)\n",
    "            mus.append(mu.cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "    mus = np.concatenate(mus, axis=0)\n",
    "    labels = np.concatenate(labels_list, axis=0)\n",
    "    return mus, labels\n",
    "\n",
    "# Extract mu vectors from test set for VAE\n",
    "print(\"\\nExtracting latent representations for VAE...\")\n",
    "mu_vae, labels_vae = extract_mu(vae, test_loader, conditional=False)\n",
    "\n",
    "# Extract mu vectors from test set for C-VAE\n",
    "print(\"Extracting latent representations for C-VAE...\")\n",
    "mu_cvae, labels_cvae = extract_mu(cvae, test_loader, conditional=True)\n",
    "\n",
    "# Apply t-SNE to reduce dimensions to 2D\n",
    "print(\"Applying t-SNE on VAE latent representations...\")\n",
    "tsne_vae = TSNE(n_components=2, random_state=42)\n",
    "mu_vae_2d = tsne_vae.fit_transform(mu_vae)\n",
    "\n",
    "print(\"Applying t-SNE on C-VAE latent representations...\")\n",
    "tsne_cvae = TSNE(n_components=2, random_state=42)\n",
    "mu_cvae_2d = tsne_cvae.fit_transform(mu_cvae)\n",
    "\n",
    "# Function to plot t-SNE results\n",
    "def plot_tsne(mu_2d, labels, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(mu_2d[:, 0], mu_2d[:, 1], c=labels, cmap='tab10', alpha=0.6, s=10)\n",
    "    plt.colorbar(scatter, ticks=range(10))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot t-SNE for VAE\n",
    "plot_tsne(mu_vae_2d, labels_vae, 't-SNE of VAE Latent Space')\n",
    "\n",
    "# Plot t-SNE for C-VAE\n",
    "plot_tsne(mu_cvae_2d, labels_cvae, 't-SNE of C-VAE Latent Space')\n",
    "\n",
    "# ---------------------------\n",
    "# 3.3 Comparison and Hypothesis\n",
    "# ---------------------------\n",
    "\n",
    "print(\"\\nComparison and Hypothesis:\")\n",
    "print(\"\"\"\n",
    "Upon visualizing the latent spaces using t-SNE, the VAE's manifold may show overlapping clusters with less distinct boundaries between different classes. This is because the VAE is unsupervised and doesn't explicitly use class labels during training, leading to latent representations that capture general data features without class-specific separation.\n",
    "\n",
    "In contrast, the C-VAE's manifold is expected to display more distinct and well-separated clusters corresponding to different classes. Since the C-VAE conditions on class labels during both encoding and decoding, it learns latent representations that are more discriminative with respect to the classes. This conditioning encourages the model to organize the latent space in a way that aligns with the class structure, resulting in clearer separations between different categories.\n",
    "\n",
    "**Hypothesis:** The inclusion of class labels in the C-VAE provides additional supervised information that guides the latent space to form class-specific clusters, enhancing the model's ability to differentiate between classes. This leads to more interpretable and organized latent representations compared to the standard VAE, which lacks this supervised signal.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
