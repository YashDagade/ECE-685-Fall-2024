{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Starting DCGAN Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCGAN Training Completed!\n",
      "Generated samples are saved in the 'samples' directory.\n",
      "Starting GAN as a Pre-Training Framework...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [1/20] Loss: 0.4111, Accuracy: 89.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [2/20] Loss: 0.0803, Accuracy: 97.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [3/20] Loss: 0.0489, Accuracy: 98.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [4/20] Loss: 0.0353, Accuracy: 99.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [5/20] Loss: 0.0247, Accuracy: 99.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [6/20] Loss: 0.0192, Accuracy: 99.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [7/20] Loss: 0.0151, Accuracy: 99.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [8/20] Loss: 0.0118, Accuracy: 99.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [9/20] Loss: 0.0096, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [10/20] Loss: 0.0077, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [11/20] Loss: 0.0068, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [12/20] Loss: 0.0059, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [13/20] Loss: 0.0051, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [14/20] Loss: 0.0046, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [15/20] Loss: 0.0042, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [16/20] Loss: 0.0037, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [17/20] Loss: 0.0033, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [18/20] Loss: 0.0031, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [19/20] Loss: 0.0028, Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [20/20] Loss: 0.0026, Accuracy: 100.00%\n",
      "Test Accuracy: 97.91%\n",
      "Training Subset Accuracy: 100.00%\n",
      "GAN Pre-Training Framework Completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Added tqdm import\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration and Setup\n",
    "# -----------------------------\n",
    "\n",
    "#run on mps\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Device configuration\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "latent_dim = 100\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "sample_dir = 'samples'\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# Data Loading\n",
    "# -----------------------------\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='data/',\n",
    "                               train=True,\n",
    "                               transform=transform,\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data/',\n",
    "                              train=False,\n",
    "                              transform=transform,\n",
    "                              download=True)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=2)\n",
    "\n",
    "# -----------------------------\n",
    "# Model Definitions\n",
    "# -----------------------------\n",
    "\n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = 7  # Initial size before upsampling\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256 * self.init_size * self.init_size)\n",
    "        )\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),\n",
    "            # Upsample to 14x14\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 7->14\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # Upsample to 28x28\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 14->28\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # Convolution to 32 channels\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            # Final convolution to 1 channel\n",
    "            nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()  # Output range [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(z.size(0), 256, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Input: 1 x 28 x 28\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),  # 28->14\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 64 x 14 x 14\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 14->7\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 128 x 7 x 7\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 7->4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 256 x 4 x 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),  # 4->4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 512 x 4 x 4\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * 3 * 3, 1),  # Changed from 512 * 4 * 4 to 512 * 3 * 3\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        features = self.conv_layers(img)\n",
    "        out = self.flatten(features)\n",
    "        validity = self.fc(out)\n",
    "        return validity\n",
    "\n",
    "    def feature_extractor(self, img):\n",
    "        with torch.no_grad():\n",
    "            features = self.conv_layers(img)\n",
    "            features = features.view(features.size(0), -1)\n",
    "        return features\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Loss and Optimizer\n",
    "# -----------------------------\n",
    "\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
    "\n",
    "# -----------------------------\n",
    "# Training the DCGAN\n",
    "# -----------------------------\n",
    "\n",
    "print(\"Starting DCGAN Training...\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\", leave=False)\n",
    "    for i, (imgs, _) in enumerate(epoch_bar):\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        real = imgs.to(device)\n",
    "        batch_size_curr = real.size(0)\n",
    "        valid = torch.ones(batch_size_curr, 1, device=device)\n",
    "        fake = torch.zeros(batch_size_curr, 1, device=device)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # Real images\n",
    "        output_real = discriminator(real)\n",
    "        loss_real = adversarial_loss(output_real, valid)\n",
    "\n",
    "        # Fake images\n",
    "        z = torch.randn(batch_size_curr, latent_dim, device=device)\n",
    "        gen_imgs = generator(z)\n",
    "        output_fake = discriminator(gen_imgs.detach())\n",
    "        loss_fake = adversarial_loss(output_fake, fake)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        loss_D = loss_real + loss_fake\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        generator.zero_grad()\n",
    "\n",
    "        # Generate images\n",
    "        gen_imgs = generator(z)\n",
    "        output = discriminator(gen_imgs)\n",
    "        loss_G = adversarial_loss(output, valid)  # Generator tries to make discriminator believe they are real\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        epoch_bar.set_postfix({'Loss D': loss_D.item(), 'Loss G': loss_G.item()})\n",
    "\n",
    "    # Save sampled images at the end of each epoch\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(12, latent_dim, device=device)\n",
    "        gen_imgs = generator(z)\n",
    "        gen_imgs = gen_imgs * 0.5 + 0.5  # Denormalize to [0,1]\n",
    "        grid = make_grid(gen_imgs, nrow=4)\n",
    "        save_image(grid, os.path.join(sample_dir, f\"epoch_{epoch}.png\"))\n",
    "\n",
    "print(\"DCGAN Training Completed!\")\n",
    "\n",
    "# -----------------------------\n",
    "# Generate and Save 12 Samples\n",
    "# -----------------------------\n",
    "\n",
    "def generate_samples(generator, latent_dim, num_samples=12, device='cpu', save_path='generated_samples.png'):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim, device=device)\n",
    "        gen_imgs = generator(z)\n",
    "        gen_imgs = gen_imgs * 0.5 + 0.5  # Denormalize to [0,1]\n",
    "        grid = make_grid(gen_imgs, nrow=4)\n",
    "        save_image(grid, save_path)\n",
    "    generator.train()\n",
    "\n",
    "generate_samples(generator, latent_dim, num_samples=12, device=device, save_path=os.path.join(sample_dir, \"final_samples.png\"))\n",
    "print(f\"Generated samples are saved in the '{sample_dir}' directory.\")\n",
    "\n",
    "# -----------------------------\n",
    "# GAN as a Pre-Training Framework\n",
    "# -----------------------------\n",
    "\n",
    "print(\"Starting GAN as a Pre-Training Framework...\")\n",
    "\n",
    "# 1. Modify the Discriminator to remove the final linear layer\n",
    "# We'll use the 'feature_extractor' method defined in the Discriminator class\n",
    "\n",
    "# 2. Extract features from 10% of the training set\n",
    "num_train_samples = int(0.1 * len(train_dataset))\n",
    "train_subset_indices = np.random.choice(len(train_dataset), num_train_samples, replace=False)\n",
    "train_subset = Subset(train_dataset, train_subset_indices)\n",
    "train_subset_loader = DataLoader(dataset=train_subset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=2)\n",
    "\n",
    "# 3. Extract features for the training subset\n",
    "discriminator.eval()\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(train_subset_loader, desc=\"Extracting Train Features\", leave=False):\n",
    "        imgs = imgs.to(device)\n",
    "        features = discriminator.feature_extractor(imgs)\n",
    "        train_features.append(features.cpu())\n",
    "        train_labels.append(labels)\n",
    "\n",
    "train_features = torch.cat(train_features, dim=0)\n",
    "train_labels = torch.cat(train_labels, dim=0)\n",
    "\n",
    "# 4. Extract features for the test set\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "test_loader_no_shuffle = DataLoader(dataset=test_dataset,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=False,\n",
    "                                    num_workers=2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(test_loader_no_shuffle, desc=\"Extracting Test Features\", leave=False):\n",
    "        imgs = imgs.to(device)\n",
    "        features = discriminator.feature_extractor(imgs)\n",
    "        test_features.append(features.cpu())\n",
    "        test_labels.append(labels)\n",
    "\n",
    "test_features = torch.cat(test_features, dim=0)\n",
    "test_labels = torch.cat(test_labels, dim=0)\n",
    "\n",
    "discriminator.train()\n",
    "\n",
    "# 5. Define a Linear Classifier\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=10):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "classifier = LinearClassifier(input_dim=512*3*3).to(device)  # Changed from 512*4*4 to 512*3*3\n",
    "\n",
    "# 6. Training the Linear Classifier\n",
    "classifier_optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "classifier_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create DataLoader for classifier training\n",
    "classifier_train_loader = DataLoader(dataset=torch.utils.data.TensorDataset(train_features, train_labels),\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    num_workers=2)\n",
    "\n",
    "# Training loop for classifier\n",
    "num_classifier_epochs = 20\n",
    "for epoch in range(1, num_classifier_epochs + 1):\n",
    "    classifier.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    classifier_bar = tqdm(classifier_train_loader, desc=f\"Classifier Epoch {epoch}/{num_classifier_epochs}\", leave=False)\n",
    "    for features, labels in classifier_bar:\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        classifier_optimizer.zero_grad()\n",
    "        outputs = classifier(features)\n",
    "        loss = classifier_criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        classifier_optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * features.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss /= total\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Classifier Epoch [{epoch}/{num_classifier_epochs}] Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# 7. Evaluate on Test Set\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    test_features = test_features.to(device)\n",
    "    test_labels = test_labels.to(device)\n",
    "    outputs = classifier(test_features)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = test_labels.size(0)\n",
    "    correct = (predicted == test_labels).sum().item()\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# 8. Evaluate on Training Subset\n",
    "with torch.no_grad():\n",
    "    train_features = train_features.to(device)\n",
    "    train_labels = train_labels.to(device)\n",
    "    outputs = classifier(train_features)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = train_labels.size(0)\n",
    "    correct = (predicted == train_labels).sum().item()\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Training Subset Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "print(\"GAN Pre-Training Framework Completed!\")\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: Visualize Generated Samples\n",
    "# -----------------------------\n",
    "\n",
    "def show_generated_images(save_path):\n",
    "    grid = plt.imread(save_path)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment the line below to visualize the final generated samples\n",
    "# show_generated_images(os.path.join(sample_dir, \"final_samples.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
