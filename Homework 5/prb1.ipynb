{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torchvision.utils as vutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(\"using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 128\n",
    "image_size=28\n",
    "nz = 100\n",
    "num_epochs=50\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "# data transformation\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "#load the MNIST dataset\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                     train=True,\n",
    "                                     transform=transform,\n",
    "                                     download=True)\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now defien the network\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.main=nn.Sequential(\n",
    "            # Input size is 1x28x28\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=4, stride=2, padding=1) , # 64x14x14\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            \n",
    "            #seocond layer\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1), # 128x7x7\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            \n",
    "            #third layer\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1), # 256x4x4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            \n",
    "            #fourth layer\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1), # 512x4x4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            \n",
    "            #output layer\n",
    "            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=0), # 1x1x1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output=self.main(input)\n",
    "        return output.view(-1)\n",
    "    \n",
    "# Generator network\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.main=nn.Sequential(\n",
    "            #Input is nz - going into convolution\n",
    "            nn.Linear(in_features=nz, out_features=256*7*7),\n",
    "            nn.BatchNorm1d(256*7*7),\n",
    "            nn.ReLU(inplace=True), \n",
    "            \n",
    "            # reshape\n",
    "            nn.Unflatten(dim=1, unflattened_size=(256,7,7)),\n",
    "            \n",
    "            # first convose transpose\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1), # 128x14x14\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second convose transpose\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1), # 64x28x28\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third convse layer\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),  # Output: (batch_size, 32, 28, 28)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1),  # Output: (batch_size, 1, 28, 28)\n",
    "            nn.Tanh()  # Output values in [-1, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Initialize the networks\n",
    "\n",
    "netD = Discriminator().to(device)\n",
    "netG = Generator().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netD.apply(weights_init)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Note that BCE is the minmax loss function under the hood\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# fixed_noise = torch.randn(64, nz, device=device)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 1/469 [00:00<01:50,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50][0/469] Loss_D: 1.5422 Loss_G: 0.4968 D(x): 0.6273 D(G(z)): 0.5722/0.6514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  43%|████▎     | 202/469 [00:44<00:56,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50][200/469] Loss_D: 1.0746 Loss_G: 0.9979 D(x): 0.4692 D(G(z)): 0.1665/0.4081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  86%|████████▌ | 401/469 [01:27<00:15,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50][400/469] Loss_D: 1.0368 Loss_G: 1.2949 D(x): 0.6768 D(G(z)): 0.4165/0.3191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 469/469 [01:42<00:00,  4.60it/s]\n",
      "Epoch 2/50:   0%|          | 1/469 [00:00<01:42,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50][0/469] Loss_D: 1.0289 Loss_G: 0.8186 D(x): 0.5280 D(G(z)): 0.2602/0.4815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50:  43%|████▎     | 202/469 [00:44<00:57,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50][200/469] Loss_D: 1.0524 Loss_G: 0.8378 D(x): 0.5436 D(G(z)): 0.3099/0.4686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50:  86%|████████▌ | 402/469 [01:28<00:14,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50][400/469] Loss_D: 1.1202 Loss_G: 1.0657 D(x): 0.6960 D(G(z)): 0.4791/0.4007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 469/469 [01:42<00:00,  4.56it/s]\n",
      "Epoch 3/50:   0%|          | 1/469 [00:00<01:44,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/50][0/469] Loss_D: 1.2017 Loss_G: 1.1515 D(x): 0.6205 D(G(z)): 0.4657/0.3533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50:  43%|████▎     | 202/469 [00:43<00:57,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/50][200/469] Loss_D: 1.2365 Loss_G: 1.3005 D(x): 0.6034 D(G(z)): 0.4677/0.3273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50:  86%|████████▌ | 402/469 [01:27<00:14,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/50][400/469] Loss_D: 1.1983 Loss_G: 0.5624 D(x): 0.4667 D(G(z)): 0.2994/0.5916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 469/469 [01:41<00:00,  4.61it/s]\n",
      "Epoch 4/50:   0%|          | 1/469 [00:00<01:44,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/50][0/469] Loss_D: 1.1684 Loss_G: 1.0299 D(x): 0.6076 D(G(z)): 0.4528/0.3884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50:  13%|█▎        | 61/469 [00:13<01:30,  4.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m errG \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[1;32m     54\u001b[0m errG\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 55\u001b[0m D_G_z2 \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     56\u001b[0m optimizerG\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Generate new fake data for the next generator iteration\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ensure the directory exists for saving images\n",
    "os.makedirs('mid_run_samples', exist_ok=True)\n",
    "\n",
    "# Fixed noise for generating samples\n",
    "fixed_noise = torch.randn(12, nz, device=device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Save generated images at the start of each epoch\n",
    "    with torch.no_grad():\n",
    "        fake_images = netG(fixed_noise).detach().cpu()\n",
    "    vutils.save_image(\n",
    "        fake_images,\n",
    "        f'mid_run_samples/output_epoch_{epoch}.png',\n",
    "        normalize=True,\n",
    "        nrow=4\n",
    "    )\n",
    "\n",
    "    # Progress bar for batches\n",
    "    for i, data in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ############################\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full(\n",
    "            (b_size,), real_label, dtype=torch.float, device=device\n",
    "        )\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Train with all-fake batch\n",
    "        noise = torch.randn(b_size, nz, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ############################\n",
    "        for _ in range(3):  # Perform 3 generator iterations so generator and discriminator are more balanced\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # Fake labels are real for generator cost\n",
    "            output = netD(fake)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimizerG.step()\n",
    "            # Generate new fake data for the next generator iteration\n",
    "            noise = torch.randn(b_size, nz, device=device)\n",
    "            fake = netG(noise)\n",
    "\n",
    "        # Print training stats\n",
    "        if i % 200 == 0:\n",
    "            print(\n",
    "                f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] '\n",
    "                f'Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} '\n",
    "                f'D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f}/{D_G_z2:.4f}'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Train Loss: 0.2956 Train Acc: 90.77% Test Loss: 0.1189 Test Acc: 96.31%\n",
      "[2/10] Train Loss: 0.0918 Train Acc: 97.07% Test Loss: 0.1149 Test Acc: 96.32%\n",
      "[3/10] Train Loss: 0.0550 Train Acc: 98.42% Test Loss: 0.0867 Test Acc: 97.30%\n",
      "[4/10] Train Loss: 0.0307 Train Acc: 99.25% Test Loss: 0.0807 Test Acc: 97.38%\n",
      "[5/10] Train Loss: 0.0169 Train Acc: 99.70% Test Loss: 0.0799 Test Acc: 97.53%\n",
      "[6/10] Train Loss: 0.0124 Train Acc: 99.92% Test Loss: 0.0745 Test Acc: 97.63%\n",
      "[7/10] Train Loss: 0.0107 Train Acc: 99.88% Test Loss: 0.0759 Test Acc: 97.55%\n",
      "[8/10] Train Loss: 0.0078 Train Acc: 99.95% Test Loss: 0.0717 Test Acc: 97.75%\n",
      "[9/10] Train Loss: 0.0057 Train Acc: 99.98% Test Loss: 0.0737 Test Acc: 97.67%\n",
      "[10/10] Train Loss: 0.0045 Train Acc: 100.00% Test Loss: 0.0725 Test Acc: 97.76%\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Part 1.2: GAN as a Pre-Training Framework\n",
    "# ------------------------------\n",
    "\n",
    "# Create a feature extractor from the discriminator\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, discriminator):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        # Copy layers up to before the last Conv2d layer\n",
    "        self.features = nn.Sequential(*list(discriminator.main.children())[:-2])  # Exclude last Conv2d and Sigmoid\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return x\n",
    "\n",
    "# Instantiate the feature extractor\n",
    "feature_extractor = FeatureExtractor(netD).to(device)\n",
    "\n",
    "# Freeze feature extractor parameters\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Prepare 10% of the training data\n",
    "train_indices = np.arange(len(dataset))\n",
    "np.random.shuffle(train_indices)\n",
    "subset_indices = train_indices[:len(dataset) // 10]\n",
    "train_subset = Subset(dataset, subset_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Prepare test data\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Determine the size of the feature vector\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(1, 1, 28, 28, device=device)\n",
    "    features = feature_extractor(dummy_input)\n",
    "    feature_size = features.shape[1]\n",
    "\n",
    "# Define a linear classifier\n",
    "classifier = nn.Linear(feature_size, 10).to(device)\n",
    "\n",
    "# Loss function and optimizer for classifier\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "optimizer_cls = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Train the classifier\n",
    "num_epochs_cls = 10\n",
    "for epoch in range(num_epochs_cls):\n",
    "    classifier.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = feature_extractor(inputs)\n",
    "        outputs = classifier(features)\n",
    "        loss = criterion_cls(outputs, labels)\n",
    "\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100. * correct / total\n",
    "\n",
    "    # Evaluate on test set\n",
    "    classifier.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            features = feature_extractor(inputs)\n",
    "            outputs = classifier(features)\n",
    "            loss = criterion_cls(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = 100. * correct_test / total_test\n",
    "\n",
    "    print(f'[{epoch + 1}/{num_epochs_cls}] Train Loss: {train_loss:.4f} Train Acc: {train_acc:.2f}% '\n",
    "          f'Test Loss: {test_loss:.4f} Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
