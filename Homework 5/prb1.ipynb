{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torchvision.utils as vutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(\"using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 128\n",
    "image_size=28\n",
    "nz = 100\n",
    "num_epochs=10\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "# data transformation\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "#load the MNIST dataset\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                     train=True,\n",
    "                                     transform=transform,\n",
    "                                     download=True)\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now defien the network\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.main=nn.Sequential(\n",
    "            # Input size is 1x28x28\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=4, stride=2, padding=1) , # 64x14x14\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            \n",
    "            #seocond layer\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1), # 128x7x7\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            \n",
    "            #third layer\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1), # 256x4x4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            \n",
    "            #fourth layer\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1), # 512x4x4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            \n",
    "            #output layer\n",
    "            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=0), # 1x1x1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output=self.main(input)\n",
    "        return output.view(-1)\n",
    "    \n",
    "# Generator network\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.main=nn.Sequential(\n",
    "            #Input is nz - going into convolution\n",
    "            nn.Linear(in_features=nz, out_features=256*7*7),\n",
    "            nn.BatchNorm1d(256*7*7),\n",
    "            nn.ReLU(inplace=True), \n",
    "            \n",
    "            # reshape\n",
    "            nn.Unflatten(dim=1, unflattened_size=(256,7,7)),\n",
    "            \n",
    "            # first convose transpose\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1), # 128x14x14\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second convose transpose\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1), # 64x28x28\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third convse layer\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),  # Output: (batch_size, 32, 28, 28)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1),  # Output: (batch_size, 1, 28, 28)\n",
    "            nn.Tanh()  # Output values in [-1, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Initialize the networks\n",
    "\n",
    "netD = Discriminator().to(device)\n",
    "netG = Generator().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netD.apply(weights_init)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Note that BCE is the minmax loss function under the hood\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# fixed_noise = torch.randn(64, nz, device=device)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 1/469 [00:00<05:19,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][0/469] Loss_D: 1.5260 Loss_G: 0.0690 D(x): 0.4970 D(G(z)): 0.4442/0.9360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  43%|████▎     | 201/469 [00:45<01:00,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][200/469] Loss_D: 0.9269 Loss_G: 1.2368 D(x): 0.6276 D(G(z)): 0.3133/0.3353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  86%|████████▌ | 401/469 [01:28<00:15,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][400/469] Loss_D: 0.9809 Loss_G: 1.0987 D(x): 0.6070 D(G(z)): 0.3243/0.3891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 469/469 [01:43<00:00,  4.52it/s]\n",
      "Epoch 2/10:   0%|          | 1/469 [00:00<01:44,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][0/469] Loss_D: 1.0770 Loss_G: 1.1401 D(x): 0.6780 D(G(z)): 0.4472/0.3522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  43%|████▎     | 201/469 [00:44<00:59,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][200/469] Loss_D: 1.1752 Loss_G: 1.1238 D(x): 0.6520 D(G(z)): 0.4787/0.3659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  86%|████████▌ | 401/469 [01:28<00:15,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][400/469] Loss_D: 1.0961 Loss_G: 0.8552 D(x): 0.5128 D(G(z)): 0.2846/0.4630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 469/469 [01:43<00:00,  4.51it/s]\n",
      "Epoch 3/10:   0%|          | 1/469 [00:00<01:46,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][0/469] Loss_D: 1.2664 Loss_G: 0.6407 D(x): 0.4241 D(G(z)): 0.2511/0.5526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  43%|████▎     | 202/469 [00:44<00:57,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][200/469] Loss_D: 1.3116 Loss_G: 1.0966 D(x): 0.6384 D(G(z)): 0.5093/0.3748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  86%|████████▌ | 401/469 [01:28<00:15,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][400/469] Loss_D: 1.3682 Loss_G: 0.8026 D(x): 0.4850 D(G(z)): 0.4198/0.4797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 469/469 [01:43<00:00,  4.55it/s]\n",
      "Epoch 4/10:   0%|          | 1/469 [00:00<01:43,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10][0/469] Loss_D: 1.2803 Loss_G: 0.6616 D(x): 0.4177 D(G(z)): 0.2782/0.5464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  43%|████▎     | 201/469 [00:44<01:00,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10][200/469] Loss_D: 1.1791 Loss_G: 0.6774 D(x): 0.4739 D(G(z)): 0.3005/0.5362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  86%|████████▌ | 401/469 [01:28<00:15,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10][400/469] Loss_D: 1.3372 Loss_G: 0.6874 D(x): 0.4771 D(G(z)): 0.3798/0.5229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 469/469 [01:43<00:00,  4.52it/s]\n",
      "Epoch 5/10:   0%|          | 1/469 [00:00<01:44,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/10][0/469] Loss_D: 1.2439 Loss_G: 0.5059 D(x): 0.4405 D(G(z)): 0.2950/0.6311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  43%|████▎     | 201/469 [00:44<01:01,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/10][200/469] Loss_D: 1.2479 Loss_G: 0.8128 D(x): 0.5253 D(G(z)): 0.4125/0.4756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  86%|████████▌ | 401/469 [01:28<00:15,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/10][400/469] Loss_D: 1.2224 Loss_G: 0.9000 D(x): 0.5643 D(G(z)): 0.4452/0.4460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 469/469 [01:43<00:00,  4.54it/s]\n",
      "Epoch 6/10:   0%|          | 1/469 [00:00<01:43,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/10][0/469] Loss_D: 1.0695 Loss_G: 0.9076 D(x): 0.6750 D(G(z)): 0.4436/0.4328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  43%|████▎     | 201/469 [00:44<01:02,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/10][200/469] Loss_D: 1.3218 Loss_G: 0.8426 D(x): 0.5257 D(G(z)): 0.4333/0.4573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  86%|████████▌ | 402/469 [01:28<00:14,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/10][400/469] Loss_D: 1.2673 Loss_G: 1.0910 D(x): 0.6936 D(G(z)): 0.5596/0.3627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 469/469 [01:42<00:00,  4.55it/s]\n",
      "Epoch 7/10:   0%|          | 1/469 [00:00<01:42,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10][0/469] Loss_D: 1.4921 Loss_G: 1.4873 D(x): 0.7663 D(G(z)): 0.6733/0.2507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  43%|████▎     | 202/469 [00:44<00:57,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10][200/469] Loss_D: 1.1786 Loss_G: 0.9075 D(x): 0.5619 D(G(z)): 0.4178/0.4275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  86%|████████▌ | 401/469 [01:28<00:15,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10][400/469] Loss_D: 1.2208 Loss_G: 1.0456 D(x): 0.6263 D(G(z)): 0.4947/0.3821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 469/469 [01:43<00:00,  4.53it/s]\n",
      "Epoch 8/10:   0%|          | 1/469 [00:00<01:42,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/10][0/469] Loss_D: 1.2661 Loss_G: 0.8924 D(x): 0.5738 D(G(z)): 0.4644/0.4396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:  43%|████▎     | 201/469 [00:44<01:00,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/10][200/469] Loss_D: 1.0331 Loss_G: 0.9138 D(x): 0.6473 D(G(z)): 0.4119/0.4377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:  86%|████████▌ | 402/469 [01:28<00:14,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/10][400/469] Loss_D: 1.1737 Loss_G: 0.8117 D(x): 0.5444 D(G(z)): 0.3932/0.4751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 469/469 [01:43<00:00,  4.55it/s]\n",
      "Epoch 9/10:   0%|          | 1/469 [00:00<01:44,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10][0/469] Loss_D: 1.4433 Loss_G: 0.5797 D(x): 0.3652 D(G(z)): 0.2733/0.5824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:  43%|████▎     | 201/469 [00:44<00:59,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10][200/469] Loss_D: 1.4778 Loss_G: 0.5023 D(x): 0.3515 D(G(z)): 0.2879/0.6282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:  86%|████████▌ | 401/469 [01:28<00:15,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10][400/469] Loss_D: 1.0848 Loss_G: 0.8698 D(x): 0.6425 D(G(z)): 0.4444/0.4449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 469/469 [01:43<00:00,  4.55it/s]\n",
      "Epoch 10/10:   0%|          | 1/469 [00:00<01:44,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/10][0/469] Loss_D: 1.1577 Loss_G: 0.7779 D(x): 0.5069 D(G(z)): 0.3360/0.4852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  43%|████▎     | 201/469 [00:44<01:01,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/10][200/469] Loss_D: 1.2193 Loss_G: 1.3009 D(x): 0.6103 D(G(z)): 0.4684/0.3078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  86%|████████▌ | 401/469 [01:28<00:15,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/10][400/469] Loss_D: 1.1723 Loss_G: 1.2154 D(x): 0.6885 D(G(z)): 0.5119/0.3318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 469/469 [01:43<00:00,  4.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Ensure the directory exists for saving images\n",
    "os.makedirs('mid_run_samples', exist_ok=True)\n",
    "\n",
    "# Fixed noise for generating samples\n",
    "fixed_noise = torch.randn(12, nz, device=device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Save generated images at the start of each epoch\n",
    "    with torch.no_grad():\n",
    "        fake_images = netG(fixed_noise).detach().cpu()\n",
    "    vutils.save_image(\n",
    "        fake_images,\n",
    "        f'mid_run_samples/output_epoch_{epoch}.png',\n",
    "        normalize=True,\n",
    "        nrow=4\n",
    "    )\n",
    "\n",
    "    # Progress bar for batches\n",
    "    for i, data in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ############################\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full(\n",
    "            (b_size,), real_label, dtype=torch.float, device=device\n",
    "        )\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Train with all-fake batch\n",
    "        noise = torch.randn(b_size, nz, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ############################\n",
    "        for _ in range(3):  # Perform 3 generator iterations so generator and discriminator are more balanced\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # Fake labels are real for generator cost\n",
    "            output = netD(fake)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimizerG.step()\n",
    "            # Generate new fake data for the next generator iteration\n",
    "            noise = torch.randn(b_size, nz, device=device)\n",
    "            fake = netG(noise)\n",
    "\n",
    "        # Print training stats\n",
    "        if i % 200 == 0:\n",
    "            print(\n",
    "                f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] '\n",
    "                f'Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} '\n",
    "                f'D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f}/{D_G_z2:.4f}'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Train Loss: 0.2568 Train Acc: 92.22% Test Loss: 0.0939 Test Acc: 97.18%\n",
      "[2/10] Train Loss: 0.0708 Train Acc: 97.72% Test Loss: 0.0896 Test Acc: 97.08%\n",
      "[3/10] Train Loss: 0.0417 Train Acc: 98.73% Test Loss: 0.0665 Test Acc: 98.04%\n",
      "[4/10] Train Loss: 0.0171 Train Acc: 99.67% Test Loss: 0.0651 Test Acc: 97.95%\n",
      "[5/10] Train Loss: 0.0106 Train Acc: 99.92% Test Loss: 0.0632 Test Acc: 98.04%\n",
      "[6/10] Train Loss: 0.0069 Train Acc: 99.97% Test Loss: 0.0623 Test Acc: 98.13%\n",
      "[7/10] Train Loss: 0.0051 Train Acc: 100.00% Test Loss: 0.0613 Test Acc: 98.05%\n",
      "[8/10] Train Loss: 0.0039 Train Acc: 100.00% Test Loss: 0.0607 Test Acc: 98.17%\n",
      "[9/10] Train Loss: 0.0033 Train Acc: 100.00% Test Loss: 0.0607 Test Acc: 98.11%\n",
      "[10/10] Train Loss: 0.0028 Train Acc: 100.00% Test Loss: 0.0598 Test Acc: 98.15%\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Part 1.2: GAN as a Pre-Training Framework\n",
    "# ------------------------------\n",
    "\n",
    "# Create a feature extractor from the discriminator\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, discriminator):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        # Copy layers up to before the last Conv2d layer\n",
    "        self.features = nn.Sequential(*list(discriminator.main.children())[:-2])  # Exclude last Conv2d and Sigmoid\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return x\n",
    "\n",
    "# Instantiate the feature extractor\n",
    "feature_extractor = FeatureExtractor(netD).to(device)\n",
    "\n",
    "# Freeze feature extractor parameters\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Prepare 10% of the training data\n",
    "train_indices = np.arange(len(dataset))\n",
    "np.random.shuffle(train_indices)\n",
    "subset_indices = train_indices[:len(dataset) // 10]\n",
    "train_subset = Subset(dataset, subset_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Prepare test data\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Determine the size of the feature vector\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(1, 1, 28, 28, device=device)\n",
    "    features = feature_extractor(dummy_input)\n",
    "    feature_size = features.shape[1]\n",
    "\n",
    "# Define a linear classifier\n",
    "classifier = nn.Linear(feature_size, 10).to(device)\n",
    "\n",
    "# Loss function and optimizer for classifier\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "optimizer_cls = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Train the classifier\n",
    "num_epochs_cls = 10\n",
    "for epoch in range(num_epochs_cls):\n",
    "    classifier.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = feature_extractor(inputs)\n",
    "        outputs = classifier(features)\n",
    "        loss = criterion_cls(outputs, labels)\n",
    "\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100. * correct / total\n",
    "\n",
    "    # Evaluate on test set\n",
    "    classifier.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            features = feature_extractor(inputs)\n",
    "            outputs = classifier(features)\n",
    "            loss = criterion_cls(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = 100. * correct_test / total_test\n",
    "\n",
    "    print(f'[{epoch + 1}/{num_epochs_cls}] Train Loss: {train_loss:.4f} Train Acc: {train_acc:.2f}% '\n",
    "          f'Test Loss: {test_loss:.4f} Test Acc: {test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of Results\n",
    "\n",
    "The trained classifier achieved a **training accuracy of 100%** and a **test accuracy of approximately 98.15%**. These high accuracy rates indicate that the feature extractor derived from the GAN effectively captures significant discriminative features from the MNIST dataset. The minimal gap between training and testing accuracies suggests that the model generalizes well to unseen data, demonstrating robust performance without evident overfitting. \n",
    "\n",
    "However, the slight decrease in test accuracy from **98.17%** to **98.15%** across epochs whiel having 100% on training accuracy may hint at the beginning of overfitting, although the change is minimal and within a negligible range. It would be intersting to consider training a simpler model - perhaps this feature extractor has too many parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note to look at samples look at mid_run_samples directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
