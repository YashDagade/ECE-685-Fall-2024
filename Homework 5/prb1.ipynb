{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torchvision.utils as vutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(\"using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 128\n",
    "image_size=28\n",
    "nz = 100\n",
    "num_epochs=50\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "# data transformation\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "#load the MNIST dataset\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                     train=True,\n",
    "                                     transform=transform,\n",
    "                                     download=True)\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now defien the network\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.main=nn.Sequential(\n",
    "            # Input size is 1x28x28\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=4, stride=2, padding=1) , # 64x14x14\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            \n",
    "            #seocond layer\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1), # 128x7x7\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            \n",
    "            #third layer\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1), # 256x4x4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            \n",
    "            #fourth layer\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1), # 512x4x4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            \n",
    "            #output layer\n",
    "            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=0), # 1x1x1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output=self.main(input)\n",
    "        return output.view(-1)\n",
    "    \n",
    "# Generator network\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.main=nn.Sequential(\n",
    "            #Input is nz - going into convolution\n",
    "            nn.Linear(in_features=nz, out_features=256*7*7),\n",
    "            nn.BatchNorm1d(256*7*7),\n",
    "            nn.ReLU(inplace=True), \n",
    "            \n",
    "            # reshape\n",
    "            nn.Unflatten(dim=1, unflattened_size=(256,7,7)),\n",
    "            \n",
    "            # first convose transpose\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1), # 128x14x14\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second convose transpose\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1), # 64x28x28\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third convse layer\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),  # Output: (batch_size, 32, 28, 28)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1),  # Output: (batch_size, 1, 28, 28)\n",
    "            nn.Tanh()  # Output values in [-1, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Initialize the networks\n",
    "\n",
    "netD = Discriminator().to(device)\n",
    "netG = Generator().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netD.apply(weights_init)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Note that BCE is the minmax loss function under the hood\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# fixed_noise = torch.randn(64, nz, device=device)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 1/469 [00:00<02:21,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50][0/469] Loss_D: 1.7309 Loss_G: 0.1947 D(x): 0.4639 D(G(z)): 0.4994/0.8380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   3%|â–Ž         | 14/469 [00:03<01:48,  4.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m errG \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[1;32m     54\u001b[0m errG\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 55\u001b[0m D_G_z2 \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     56\u001b[0m optimizerG\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Generate new fake data for the next generator iteration\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ensure the directory exists for saving images\n",
    "os.makedirs('mid_run_samples', exist_ok=True)\n",
    "\n",
    "# Fixed noise for generating samples\n",
    "fixed_noise = torch.randn(12, nz, device=device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Save generated images at the start of each epoch\n",
    "    with torch.no_grad():\n",
    "        fake_images = netG(fixed_noise).detach().cpu()\n",
    "    vutils.save_image(\n",
    "        fake_images,\n",
    "        f'mid_run_samples/output_epoch_{epoch}.png',\n",
    "        normalize=True,\n",
    "        nrow=4\n",
    "    )\n",
    "\n",
    "    # Progress bar for batches\n",
    "    for i, data in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ############################\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full(\n",
    "            (b_size,), real_label, dtype=torch.float, device=device\n",
    "        )\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Train with all-fake batch\n",
    "        noise = torch.randn(b_size, nz, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ############################\n",
    "        for _ in range(3):  # Perform 3 generator iterations so generator and discriminator are more balanced\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # Fake labels are real for generator cost\n",
    "            output = netD(fake)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimizerG.step()\n",
    "            # Generate new fake data for the next generator iteration\n",
    "            noise = torch.randn(b_size, nz, device=device)\n",
    "            fake = netG(noise)\n",
    "\n",
    "        # Print training stats\n",
    "        if i % 200 == 0:\n",
    "            print(\n",
    "                f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] '\n",
    "                f'Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} '\n",
    "                f'D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f}/{D_G_z2:.4f}'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright now using the discriminator as a feature extractor and than doing classification on it\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, discriminator):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        # copy all but the sigmoid layer \n",
    "        self.features=nn.Sequential(*list(discriminator.main.children())[:-2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buddy \n",
      " Dog\n"
     ]
    }
   ],
   "source": [
    "class Animal:\n",
    "    def __init__(self,name,species):\n",
    "        self.name = name\n",
    "        self.species = species\n",
    "        \n",
    "dog = Animal(\"Buddy\", \"Dog\")\n",
    "print(dog.name, \"\\n\", dog.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
