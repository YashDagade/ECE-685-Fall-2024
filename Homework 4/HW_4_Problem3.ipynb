{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Extracting H0 (Random Projection) Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 391/391 [00:02<00:00, 151.00it/s]\n",
      "Extracting Features: 100%|██████████| 79/79 [00:00<00:00, 150.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H0_train shape: torch.Size([50000, 128])\n",
      "H0_test shape: torch.Size([10000, 128])\n",
      "\n",
      "Starting Autoencoder Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]: 100%|██████████| 391/391 [00:05<00:00, 72.29it/s, loss=0.0144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/20]: 100%|██████████| 391/391 [00:05<00:00, 73.55it/s, loss=0.0107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/20]: 100%|██████████| 391/391 [00:05<00:00, 70.97it/s, loss=0.00921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 0.0097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/20]: 100%|██████████| 391/391 [00:05<00:00, 74.44it/s, loss=0.00789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 0.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/20]: 100%|██████████| 391/391 [00:06<00:00, 57.78it/s, loss=0.00722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 0.0076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/20]: 100%|██████████| 391/391 [00:06<00:00, 56.17it/s, loss=0.00646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 0.0069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/20]: 100%|██████████| 391/391 [00:06<00:00, 56.94it/s, loss=0.00566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/20]: 100%|██████████| 391/391 [00:06<00:00, 56.96it/s, loss=0.00545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/20]: 100%|██████████| 391/391 [00:06<00:00, 57.17it/s, loss=0.00507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/20]: 100%|██████████| 391/391 [00:06<00:00, 56.22it/s, loss=0.0052] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/20]: 100%|██████████| 391/391 [00:07<00:00, 54.93it/s, loss=0.00454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/20]: 100%|██████████| 391/391 [00:06<00:00, 56.39it/s, loss=0.00466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Loss: 0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/20]: 100%|██████████| 391/391 [00:06<00:00, 56.10it/s, loss=0.00435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/20]: 100%|██████████| 391/391 [00:06<00:00, 55.95it/s, loss=0.00433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/20]: 100%|██████████| 391/391 [00:06<00:00, 56.01it/s, loss=0.00374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [16/20]: 100%|██████████| 391/391 [00:06<00:00, 56.04it/s, loss=0.00378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [17/20]: 100%|██████████| 391/391 [00:06<00:00, 56.56it/s, loss=0.00385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [18/20]: 100%|██████████| 391/391 [00:06<00:00, 56.47it/s, loss=0.00333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [19/20]: 100%|██████████| 391/391 [00:07<00:00, 53.59it/s, loss=0.0037] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [20/20]: 100%|██████████| 391/391 [00:07<00:00, 53.16it/s, loss=0.0034] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 0.0036\n",
      "Finished Training Autoencoder\n",
      "\n",
      "Extracting H1 (Trained Features) Features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 391/391 [00:02<00:00, 145.90it/s]\n",
      "Extracting Features: 100%|██████████| 79/79 [00:00<00:00, 132.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1_train shape: torch.Size([50000, 128])\n",
      "H1_test shape: torch.Size([10000, 128])\n",
      "\n",
      "Training classifier on H0 (Random Projection)\n",
      "\n",
      "Starting Training for 20 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [1/20]: 100%|██████████| 196/196 [00:00<00:00, 271.87it/s, loss=2.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [2/20]: 100%|██████████| 196/196 [00:00<00:00, 302.49it/s, loss=2.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 2.1680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [3/20]: 100%|██████████| 196/196 [00:00<00:00, 311.67it/s, loss=2.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 2.1314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [4/20]: 100%|██████████| 196/196 [00:00<00:00, 299.79it/s, loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 2.1036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [5/20]: 100%|██████████| 196/196 [00:00<00:00, 331.73it/s, loss=2.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 2.0788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [6/20]: 100%|██████████| 196/196 [00:00<00:00, 244.67it/s, loss=1.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 2.0606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [7/20]: 100%|██████████| 196/196 [00:00<00:00, 266.55it/s, loss=2.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 2.0468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [8/20]: 100%|██████████| 196/196 [00:00<00:00, 288.50it/s, loss=1.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Loss: 2.0353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [9/20]: 100%|██████████| 196/196 [00:00<00:00, 322.30it/s, loss=2.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Loss: 2.0259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [10/20]: 100%|██████████| 196/196 [00:00<00:00, 304.79it/s, loss=1.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Loss: 2.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [11/20]: 100%|██████████| 196/196 [00:00<00:00, 311.02it/s, loss=1.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Loss: 2.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [12/20]: 100%|██████████| 196/196 [00:00<00:00, 278.82it/s, loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Loss: 2.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [13/20]: 100%|██████████| 196/196 [00:00<00:00, 321.32it/s, loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 1.9957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [14/20]: 100%|██████████| 196/196 [00:00<00:00, 276.89it/s, loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 1.9890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [15/20]: 100%|██████████| 196/196 [00:00<00:00, 309.27it/s, loss=1.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Loss: 1.9825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [16/20]: 100%|██████████| 196/196 [00:00<00:00, 299.68it/s, loss=1.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 1.9765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [17/20]: 100%|██████████| 196/196 [00:00<00:00, 323.53it/s, loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 1.9698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [18/20]: 100%|██████████| 196/196 [00:00<00:00, 281.54it/s, loss=1.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 1.9635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [19/20]: 100%|██████████| 196/196 [00:00<00:00, 317.46it/s, loss=2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 1.9567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [20/20]: 100%|██████████| 196/196 [00:00<00:00, 315.46it/s, loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 1.9506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 40/40 [00:00<00:00, 816.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Accuracy on H0: 29.44%\n",
      "\n",
      "Training classifier on H1 (Trained Features)\n",
      "\n",
      "Starting Training for 20 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [1/20]: 100%|██████████| 196/196 [00:00<00:00, 314.83it/s, loss=1.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.8496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [2/20]: 100%|██████████| 196/196 [00:00<00:00, 325.28it/s, loss=1.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 1.6423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [3/20]: 100%|██████████| 196/196 [00:00<00:00, 303.96it/s, loss=1.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 1.5620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [4/20]: 100%|██████████| 196/196 [00:00<00:00, 302.97it/s, loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 1.5165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [5/20]: 100%|██████████| 196/196 [00:00<00:00, 305.32it/s, loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 1.4892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [6/20]: 100%|██████████| 196/196 [00:00<00:00, 334.53it/s, loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 1.4669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [7/20]: 100%|██████████| 196/196 [00:00<00:00, 339.33it/s, loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 1.4489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [8/20]: 100%|██████████| 196/196 [00:00<00:00, 321.13it/s, loss=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Loss: 1.4315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [9/20]: 100%|██████████| 196/196 [00:00<00:00, 336.92it/s, loss=1.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Loss: 1.4194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [10/20]: 100%|██████████| 196/196 [00:00<00:00, 290.16it/s, loss=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Loss: 1.4051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [11/20]: 100%|██████████| 196/196 [00:00<00:00, 327.52it/s, loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Loss: 1.3949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [12/20]: 100%|██████████| 196/196 [00:00<00:00, 311.20it/s, loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Loss: 1.3866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [13/20]: 100%|██████████| 196/196 [00:00<00:00, 322.56it/s, loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 1.3791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [14/20]: 100%|██████████| 196/196 [00:00<00:00, 307.05it/s, loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 1.3697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [15/20]: 100%|██████████| 196/196 [00:00<00:00, 301.03it/s, loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Loss: 1.3654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [16/20]: 100%|██████████| 196/196 [00:00<00:00, 282.80it/s, loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 1.3570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [17/20]: 100%|██████████| 196/196 [00:00<00:00, 332.12it/s, loss=1.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 1.3535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [18/20]: 100%|██████████| 196/196 [00:00<00:00, 306.13it/s, loss=1.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 1.3488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [19/20]: 100%|██████████| 196/196 [00:00<00:00, 311.33it/s, loss=1.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 1.3442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch [20/20]: 100%|██████████| 196/196 [00:00<00:00, 307.99it/s, loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 1.3394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 40/40 [00:00<00:00, 812.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Accuracy on H1: 50.15%\n",
      "\n",
      "--- Comparison of Classifier Performance ---\n",
      "Accuracy with H0 (Random Projection): 29.44%\n",
      "Accuracy with H1 (Trained Features): 50.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check and set the device (CPU, CUDA, or MPS)\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Data Preparation\n",
    "# -----------------------------\n",
    "\n",
    "# Define transformations: downsample to 16x16 and convert images to tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((16, 16)),  # Downsample images for faster computation\n",
    "    transforms.ToTensor(),        # Convert PIL images to tensors\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and test datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "# Define DataLoaders with num_workers=0 to avoid multiprocessing issues\n",
    "batch_size = 128  # Batch size for training the autoencoder\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True, num_workers=0)  # Set num_workers=0\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                         shuffle=False, num_workers=0)  # Set num_workers=0\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Define Autoencoder Architecture\n",
    "# -----------------------------\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: 3 Convolutional layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),  # Output: 32 x 8 x 8\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # Output: 64 x 4 x 4\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),# Output: 128 x 2 x 2\n",
    "            nn.ReLU(True),\n",
    "            nn.Flatten(),                                         # Flatten to 512 (128*2*2)\n",
    "            nn.Linear(128*2*2, latent_dim)                        # Latent vector\n",
    "        )\n",
    "        # Decoder: 3 Transposed Convolutional layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128*2*2),                       # Expand latent vector\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (128, 2, 2)),                         # Reshape to 128 x 2 x 2\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1), # 4x4\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # 8x8\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=3, stride=2, padding=1, output_padding=1),   # 16x16\n",
    "            nn.Sigmoid()                                           # Output values between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)  # Encode input to latent space\n",
    "        reconstructed = self.decoder(latent)  # Decode latent vector to reconstruct input\n",
    "        return reconstructed\n",
    "\n",
    "# Initialize the autoencoder and move it to the appropriate device\n",
    "autoencoder = Autoencoder(latent_dim=128).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Feature Extraction Function\n",
    "# -----------------------------\n",
    "\n",
    "def extract_features(model, dataloader):\n",
    "    \"\"\"\n",
    "    Extract latent features from the dataset using the encoder part of the autoencoder.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The autoencoder model.\n",
    "        dataloader (DataLoader): DataLoader for the dataset.\n",
    "\n",
    "    Returns:\n",
    "        features (torch.Tensor): Extracted latent features.\n",
    "        labels (torch.Tensor): Corresponding labels.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for data, target in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "            data = data.to(device)  # Move data to device\n",
    "            latent = model.encoder(data)  # Get latent features\n",
    "            features.append(latent.cpu())  # Move features to CPU and store\n",
    "            labels.append(target)  # Store labels\n",
    "    features = torch.cat(features, dim=0)  # Concatenate all features\n",
    "    labels = torch.cat(labels, dim=0)      # Concatenate all labels\n",
    "    return features, labels\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Extract Initial Features (H0) with Random Autoencoder\n",
    "# -----------------------------\n",
    "\n",
    "print(\"Extracting H0 (Random Projection) Features...\")\n",
    "H0_train, y0_train = extract_features(autoencoder, train_loader)\n",
    "H0_test, y0_test = extract_features(autoencoder, test_loader)\n",
    "\n",
    "print(f'H0_train shape: {H0_train.shape}')\n",
    "print(f'H0_test shape: {H0_test.shape}')\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Train the Autoencoder\n",
    "# -----------------------------\n",
    "\n",
    "# Define the loss function (Mean Squared Error) and the optimizer (Adam)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "num_epochs = 20  # Number of epochs for training\n",
    "\n",
    "print(\"\\nStarting Autoencoder Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    loop = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    for data, _ in loop:\n",
    "        data = data.to(device)  # Move data to device\n",
    "\n",
    "        # Forward pass: reconstruct the input\n",
    "        reconstructed = autoencoder(data)\n",
    "        loss = criterion(reconstructed, data)  # Compute reconstruction loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        loss.backward()        # Backpropagate the loss\n",
    "        optimizer.step()       # Update the weights\n",
    "\n",
    "        running_loss += loss.item() * data.size(0)  # Accumulate loss\n",
    "        loop.set_postfix(loss=loss.item())         # Update progress bar\n",
    "\n",
    "    # Compute average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "print('Finished Training Autoencoder')\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Extract Trained Features (H1) with Trained Autoencoder\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\nExtracting H1 (Trained Features) Features...\")\n",
    "H1_train, y1_train = extract_features(autoencoder, train_loader)\n",
    "H1_test, y1_test = extract_features(autoencoder, test_loader)\n",
    "\n",
    "print(f'H1_train shape: {H1_train.shape}')\n",
    "print(f'H1_test shape: {H1_test.shape}')\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Define and Train Classifiers\n",
    "# -----------------------------\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=128, num_classes=10):\n",
    "        super(Classifier, self).__init__()\n",
    "        # Simple feedforward network with one hidden layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),  # Input layer\n",
    "            nn.ReLU(True),             # Activation\n",
    "            nn.Linear(64, num_classes) # Output layer\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)  # Forward pass\n",
    "        return out\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for handling pre-extracted features and labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets for H0 (Random Projection)\n",
    "H0_train_dataset = FeatureDataset(H0_train, y0_train)\n",
    "H0_test_dataset = FeatureDataset(H0_test, y0_test)\n",
    "\n",
    "# Create datasets for H1 (Trained Features)\n",
    "H1_train_dataset = FeatureDataset(H1_train, y1_train)\n",
    "H1_test_dataset = FeatureDataset(H1_test, y1_test)\n",
    "\n",
    "# Define DataLoaders for classifiers with num_workers=0\n",
    "classifier_batch_size = 256  # Batch size for classifier training\n",
    "\n",
    "H0_train_loader = DataLoader(H0_train_dataset, batch_size=classifier_batch_size,\n",
    "                             shuffle=True, num_workers=0)  # Set num_workers=0\n",
    "H0_test_loader = DataLoader(H0_test_dataset, batch_size=classifier_batch_size,\n",
    "                            shuffle=False, num_workers=0)  # Set num_workers=0\n",
    "\n",
    "H1_train_loader = DataLoader(H1_train_dataset, batch_size=classifier_batch_size,\n",
    "                             shuffle=True, num_workers=0)  # Set num_workers=0\n",
    "H1_test_loader = DataLoader(H1_test_dataset, batch_size=classifier_batch_size,\n",
    "                            shuffle=False, num_workers=0)  # Set num_workers=0\n",
    "\n",
    "def train_classifier(model, train_loader, test_loader, num_epochs=20, learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    Train a classifier model and evaluate its accuracy on the test set.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The classifier model to train.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        test_loader (DataLoader): DataLoader for testing data.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): Test accuracy of the trained classifier.\n",
    "    \"\"\"\n",
    "    model = model.to(device)  # Move model to device\n",
    "    criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Optimizer\n",
    "\n",
    "    print(f\"\\nStarting Training for {num_epochs} epochs...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        loop = tqdm(train_loader, desc=f'Classifier Epoch [{epoch+1}/{num_epochs}]')\n",
    "        for features, labels in loop:\n",
    "            features = features.to(device)  # Move features to device\n",
    "            labels = labels.to(device)      # Move labels to device\n",
    "\n",
    "            # Forward pass: compute predictions\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            loss.backward()        # Backpropagate the loss\n",
    "            optimizer.step()       # Update the weights\n",
    "\n",
    "            running_loss += loss.item() * features.size(0)  # Accumulate loss\n",
    "            loop.set_postfix(loss=loss.item())             # Update progress bar\n",
    "\n",
    "        # Compute average loss for the epoch\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for features, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            features = features.to(device)  # Move features to device\n",
    "            labels = labels.to(device)      # Move labels to device\n",
    "            outputs = model(features)       # Get model predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get predicted classes\n",
    "            total += labels.size(0)          # Total number of samples\n",
    "            correct += (predicted == labels).sum().item()  # Correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Calculate accuracy\n",
    "    return accuracy\n",
    "\n",
    "# Initialize classifiers for H0 and H1\n",
    "classifier_H0 = Classifier(input_dim=128, num_classes=10)\n",
    "classifier_H1 = Classifier(input_dim=128, num_classes=10)\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Train and Evaluate Classifier on H0 (Random Projection)\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\nTraining classifier on H0 (Random Projection)\")\n",
    "accuracy_H0 = train_classifier(classifier_H0, H0_train_loader, H0_test_loader, num_epochs=20)\n",
    "print(f'Classifier Accuracy on H0: {accuracy_H0:.2f}%')\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Train and Evaluate Classifier on H1 (Trained Features)\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\nTraining classifier on H1 (Trained Features)\")\n",
    "accuracy_H1 = train_classifier(classifier_H1, H1_train_loader, H1_test_loader, num_epochs=20)\n",
    "print(f'Classifier Accuracy on H1: {accuracy_H1:.2f}%')\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Compare Classifier Performance\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\n--- Comparison of Classifier Performance ---\")\n",
    "print(f'Accuracy with H0 (Random Projection): {accuracy_H0:.2f}%')\n",
    "print(f'Accuracy with H1 (Trained Features): {accuracy_H1:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
