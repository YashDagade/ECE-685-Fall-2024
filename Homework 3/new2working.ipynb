{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2686 Testing samples: 664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/111 [00:00<?, ?batch/s]/opt/anaconda3/lib/python3.12/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.16). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.16). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.16). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.16). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "Epoch 1/10:  90%|█████████ | 100/111 [00:49<00:04,  2.54batch/s, loss=194]"
     ]
    }
   ],
   "source": [
    "# Section 1.2: Object Detection with Pre-trained Feature Extractor\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from datasets import FacesDataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "# Load the CSV file\n",
    "bbox_data = pd.read_csv('data/faces.csv')\n",
    "\n",
    "# Get the unique image names - since each image may have multiple bounding boxes, we need to get the unique image names\n",
    "image_names = bbox_data['image_name'].unique()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_images, test_images = train_test_split(image_names, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataFrames for training and testing\n",
    "train_df = bbox_data[bbox_data['image_name'].isin(train_images)]\n",
    "test_df = bbox_data[bbox_data['image_name'].isin(test_images)]\n",
    "\n",
    "print(\"Training samples:\", len(train_df), \"Testing samples:\", len(test_df))\n",
    "\n",
    "# Load pre-trained ResNet-50 model using the updated 'weights' parameter\n",
    "backbone = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# Freeze backbone weights\n",
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "class ObjectDetectionModel(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(ObjectDetectionModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.backbone.fc = nn.Identity()  # Remove the original classification head\n",
    "        self.regressor = nn.Linear(2048, 4)  # New regression head for bounding boxes\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        bbox_preds = self.regressor(features)\n",
    "        return bbox_preds\n",
    "\n",
    "# Instantiate the model\n",
    "model = ObjectDetectionModel(backbone)\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.RandomSizedBBoxSafeCrop(width=512, height=512, erosion_rate=0.2, p=1.0),  # Ensure all images are cropped and resized to 512x512\n",
    "    A.HorizontalFlip(p=0.4),\n",
    "    A.ColorJitter(p=0.42),\n",
    "    A.RandomBrightnessContrast(p=0.4),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])) # to take care of bounding boxes\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = FacesDataset(train_df, image_dir='data/images', transform=transform)\n",
    "test_dataset = FacesDataset(test_df, image_dir='data/images', transform=transform)\n",
    "\n",
    "# Prepare DataLoader\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 5  # Set to 0 to avoid the FacesDataset error in Jupyter - could be parallelized in a script\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.SmoothL1Loss()  # Also known as Huber loss\n",
    "optimizer = torch.optim.Adam(model.regressor.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# Determine the device - because I am thinking of either running on my mac or the cloud nodes\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Integrate mixed precision training if using CUDA\n",
    "use_amp = torch.cuda.is_available()\n",
    "\n",
    "if use_amp:\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    for images, targets in progress_bar:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)  # Shape: [batch_size, 4]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)  # Shape: [batch_size, 4]\n",
    "                loss = criterion(outputs, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {epoch_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
