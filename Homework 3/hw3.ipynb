{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"sbaghbidi/human-faces-object-detection\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.16). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, annotations, images_dir, transform=None):\n",
    "        self.annotations = annotations  # No need to reset index here\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Use the original indices to access the DataFrame rows\n",
    "        img_name = self.annotations.iloc[idx]['image_name']\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Get bounding box coordinates\n",
    "        bbox = self.annotations.iloc[idx][['x0', 'y0', 'x1', 'y1']].values.astype(np.float32)\n",
    "        bbox = torch.tensor(bbox)\n",
    "        \n",
    "        if self.transform:\n",
    "            image, bbox = self.transform(image, bbox)\n",
    "        \n",
    "        return image, bbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3350\n",
      "Train samples: 2680\n",
      "Validation samples: 670\n"
     ]
    }
   ],
   "source": [
    "# Path configurations\n",
    "data_dir = '/Users/yd211/Documents/GitHub/ECE 685 Fall 2024/Homework 3/data'\n",
    "images_dir = os.path.join(data_dir, 'images')\n",
    "csv_path = os.path.join(data_dir, 'faces.csv')\n",
    "\n",
    "# Load annotations\n",
    "annotations = pd.read_csv(csv_path)\n",
    "print(f\"Total samples: {len(annotations)}\")\n",
    "\n",
    "# use train test split from sklearn\n",
    "\n",
    "train_annotations, val_annotations = train_test_split(annotations, test_size=0.2, random_state=12)\n",
    "print(f\"Train samples: {len(train_annotations)}\")\n",
    "print(f\"Validation samples: {len(val_annotations)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size (resize for consistency and GPU memory constraints)\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# Define transformation pipeline\n",
    "transform = T.Compose([\n",
    "    RandomAffineTransform(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "    T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# Instantiate datasets\n",
    "train_dataset = FaceDataset(train_annotations, images_dir, transform=transform)\n",
    "test_dataset = FaceDataset(test_annotations, images_dir, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Compose.__call__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     15\u001b[0m     idx \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     image, bbox \u001b[38;5;241m=\u001b[39m train_dataset[idx]\n\u001b[1;32m     17\u001b[0m     plot_image_with_bbox(image, bbox, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 21\u001b[0m, in \u001b[0;36mFaceDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m bbox \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(bbox)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 21\u001b[0m     image, bbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image, bbox)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, bbox\n",
      "\u001b[0;31mTypeError\u001b[0m: Compose.__call__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "def plot_image_with_bbox(image, bbox, title=\"\"):\n",
    "    # image: tensor\n",
    "    # bbox: tensor [x0, y0, x1, y1]\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    plt.imshow(image)\n",
    "    # Scale bbox to image size\n",
    "    x0, y0, x1, y1 = bbox\n",
    "    plt.plot([x0, x1, x1, x0, x0], [y0, y0, y1, y1, y0], 'r-', linewidth=2)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Sample and plot twice to check randomness and consistency\n",
    "for i in range(2):\n",
    "    idx = random.randint(0, len(train_dataset)-1)\n",
    "    image, bbox = train_dataset[idx]\n",
    "    plot_image_with_bbox(image, bbox, title=f\"Sample {i+1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
