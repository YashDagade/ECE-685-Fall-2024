All right let's do this:

1 Problem 1: Object Detection with Convolutional Neural Net- works (CNNs)
In this task, you will train a CNN model for a simplified human-face detection problem. The dataset contains 2204 imaegs and a .csv file for bounding box coordinate. It can be downloaded from https://www.kaggle.com/datasets/sbaghbidi/human-faces-object-detection/data.
Please divide your dataset into training and testing sets with a ratio of 4:1. After training, evaluate the model’s performance on the test set.
1.1 Data Preprocessing (15 points)
Typically, bounding box annotations for object detection tasks are provided in formats like .xml, .json, or .csv. Your task is to implement a custom dataset class for object detection by inheriting from the ‘torch.utils.data.Dataset‘ class. The dataset should return an image along with its corresponding bounding box when the ‘ getitem ‘ function is called. Additionally, a spatial transformations (e.g., random affine transformations, random elastic deformations) that should be applied to both the image and its bounding box as part of the function call. You may use any library to accomplish this task. After implementing the dataset, sample from the dataset you implemented and visualize the bounding boxes overlaid on the images. Verify that the bounding boxes are correctly positioned and that any spatial transformations are consistently applied to both the images and their respective bounding boxes. Sample and plot twice to make sure that your augmentations are indeed random and consistent.
1.2 Object Detection with Pre-trained Feature Extractor(25 pts)
Once your dataset is ready, you can download a pre-trained classifier (e.g., one trained on ImageNet) from any source to use as your feature extraction backbone. This backbone will be used to extract latent representations from your images. These latent representations will then serve as the input to a module that you will train to predict the coordinates (or transformed coordinates) of the bounding boxes. Note that the weights of the feature extraction backbone should remain frozen and must not be modified during this process. Evaluate the performance of your bounding box regression network on the test set using IOU (see Fig. 1) as the metric and plot a few examples of your output.
1.3 Object Detection with End-to-End Fine-Tuning(15 pts)
Next, unfreeze the weights of your feature extraction network and ensure that the feature extraction backbone and the bounding box regression module are connected with gradients. Train both the back- bone and the bounding box prediction module together and observe if there is any improvement in model performance. Finally, report the Intersection over Union (IoU) for the model and visualize a few exam- ples of your bounding box predictions.
Note: For simplicity, most images in this dataset are selected to contain just one object of interest. It’s ok to have the bounding box regressor to predict only one bounding box. However, think carefully how do you want to represent your bounding box. If the image sizes are too larger to fit into the GPU memory, consider using ”torch.cuda.amp.autocast()” (or the mac equivalent) or reducing your image size and batch size
1.4 What if there are multiple objects? (10pts)
In the previous dataset, we assumed that each image contains exactly one object. Would your current model still work if there were zero or multiple objects from different categories in the images? Propose a modification that would enable your model to handle such cases and explain how this adjustment addresses or mitigates the issue. List relevant literature when applicable.




I have downloaded the dataset with this command and put it in the Homework 3 Repo under the data folder. The data folder has an images folder and a faces.csv folder which is as such:

image_name,width,height,x0,y0,x1,y1
00001722.jpg,1333,2000,490,320,687,664
00001044.jpg,2000,1333,791,119,1200,436
00001050.jpg,667,1000,304,155,407,331
00001736.jpg,626,417,147,14,519,303


and the images folder is just these images


I also have a HW 3.ipynb which is outside the data folder but in the Homework 3 Folder

Note that each iamge may have multiple faces and that's why there may be multiple entries per image in the faces.csv file such as this example which has 3 faces in this iamge:

00003358.jpg,800,533,235,173,406,305
00003358.jpg,800,533,517,262,632,352
00003358.jpg,800,533,410,190,519,275

Also note that all the images are jpg and not the same width and height or the aspect ratio. 

When working on creating a faces dataset please use these libraries : from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split to creates a facesdatfaset class and when working on random affine transformations please use from albumentations.pytorch import ToTensorV2

also note that you will have to make sure that your code applies the same transformations to the bounding box cordinates too and that your code is quite fast. 


Below is some code that just visualizes 3 random images from the dataset - it dosen't use the needed libraries and has no transformation, you will have to optimzie it adn make it better, however I have included it for your understanding and refrence below:


def display_image_with_bboxes(image_path, bboxes):
    # Open the image
    image = Image.open(image_path)
    fig, ax = plt.subplots(1)
    ax.imshow(image)
    
    # Add bounding boxes
    for bbox in bboxes:
        x0, y0, x1, y1 = bbox
        rect = patches.Rectangle((x0, y0), x1 - x0, y1 - y0, linewidth=1, edgecolor='r', facecolor='none')
        ax.add_patch(rect)
    
    plt.show()

# Group bounding boxes by image filename
grouped = bbox_data.groupby('image_name')

# Randomly select 2 or 3 images
selected_filenames = random.sample(list(grouped.groups.keys()), 3)

# Iterate through each selected image and display it with its bounding boxes
for filename in selected_filenames:
    image_path = os.path.join('data', 'images', filename)
    print(f"Displaying image: {image_path}")
    bboxes = grouped.get_group(filename)[['x0', 'y0', 'x1', 'y1']].values
    display_image_with_bboxes(image_path, bboxes)


Also for section 1.1 note that when you apply a random affine transformatino you may be wondering if the boundign box becomes a parallelogram. The answer is no - You do not have to compute the parallelogram, simply consider making a new bounding box which tightly bound the old bounding box would be enough and make it a rectangel with it's lines being parallel to the images

For Section 1.2 - We will use a pretrained classifier - import it from torchvision.models 

Make sure that your code is fully correct and try to optimize it. Split your code in to 3 cells - for section 1.1, 1.2 and than for 1.3 sequentially!






_____


Okay grat - I modifeid the seciton 1.1 code a bit, we were getting an error before so I fixed that and chagned a few more things - this is all of the 1.1 code now:

import pandas as pd
import numpy as np
import os
import random
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches

import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

import albumentations as A
from albumentations.pytorch import ToTensorV2


# Load the CSV file
bbox_data = pd.read_csv('data/faces.csv')

# Get the unique image names - since each image may have multiple bounding boxes, we need to get the unique image names
image_names = bbox_data['image_name'].unique()

# Split the data into training and testing sets
train_images, test_images = train_test_split(image_names, test_size=0.2, random_state=42)

# Create DataFrames for training and testing
train_df = bbox_data[bbox_data['image_name'].isin(train_images)]
test_df = bbox_data[bbox_data['image_name'].isin(test_images)]

print("Training samples:", len(train_df), "Testing samples:", len(test_df))


class FacesDataset(Dataset):
    def __init__(self, dataframe, image_dir, transform=None):
        self.dataframe = dataframe
        self.image_dir = image_dir
        self.transform = transform
        
        # Get list of unique image names
        self.image_names = self.dataframe['image_name'].unique()
        
        # Group bounding boxes by image name
        self.groups = self.dataframe.groupby('image_name')
        
    def __len__(self):
        return len(self.image_names)
    
    def __getitem__(self, idx):
        image_name = self.image_names[idx]
        img_path = os.path.join(self.image_dir, image_name)
        image = np.array(Image.open(img_path).convert("RGB"))
        
        # Get bounding boxes
        records = self.groups.get_group(image_name)
        boxes = records[['x0', 'y0', 'x1', 'y1']].values
        
        # Apply transformations
        sample = {
            'image': image,
            'bboxes': boxes,
            'labels': [0] * len(boxes)  # Dummy labels for albumentations lib
        }
        
        if self.transform:
            sample = self.transform(**sample)
        
        # Convert to tensors
        image = sample['image']
        boxes = torch.tensor(sample['bboxes'])
        
        return image, boxes


# Random Affine Transformations
transform = A.Compose([
    A.RandomSizedBBoxSafeCrop(512, 512),
    A.HorizontalFlip(p=0.4),
    #A.ShiftScaleRotate(p=0.8),
    A.ColorJitter(p=0.42),
    A.RandomBrightnessContrast(p=0.4),
    A.RandomScale(scale_limit=0.1, p=0.3),
    ToTensorV2()
], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'])) # to take care of the boudning boxes


train_dataset = FacesDataset(train_df, image_dir='data/images', transform=transform)
test_dataset = FacesDataset(test_df, image_dir='data/images', transform=transform)


def display_image_with_bboxes(image, bboxes):
    # Convert tensor image to NumPy array
    image = image.permute(1, 2, 0).cpu().numpy()

    
    fig, ax = plt.subplots(1)
    ax.imshow(image)
    
    # Plot bounding boxes
    for bbox in bboxes:
        x0, y0, x1, y1 = bbox
        rect = patches.Rectangle((x0, y0), x1 - x0, y1 - y0, linewidth=2, edgecolor='r', facecolor='none')
        ax.add_patch(rect)
    
    plt.axis('off')
    plt.show()

# Visualize two samples to ensure random and consistent augmentations
for _ in range(2):
    idx = random.randint(0, len(train_dataset) - 1)
    image, bboxes = train_dataset[idx]
    display_image_with_bboxes(image, bboxes)


Okay now for 1.2 - this is the problem:

Once your dataset is ready, you can download a pre-trained classifier (e.g., one trained on ImageNet) from any source to use as your feature extraction backbone. This backbone will be used to extract latent representations from your images. These latent representations will then serve as the input to a module that you will train to predict the coordinates (or transformed coordinates) of the bounding boxes. Note that the weights of the feature extraction backbone should remain frozen and must not be modified during this process. Evaluate the performance of your bounding box regression network on the test set using IOU (see Fig. 1) as the metric and plot a few examples of your output.


and this is our code for that:

import torchvision.models as models
import torch.nn as nn

# Load pre-trained ResNet-50 model
backbone = models.resnet50(pretrained=True)

# Freeze backbone weights
for param in backbone.parameters():
    param.requires_grad = False


class ObjectDetectionModel(nn.Module):
    def __init__(self, backbone):
        super(ObjectDetectionModel, self).__init__()
        self.backbone = backbone
        self.backbone.fc = nn.Identity()  # Remove the original classification head
        self.regressor = nn.Linear(2048, 4)  # New regression head for bounding boxes
        
    def forward(self, x):
        features = self.backbone(x)
        bbox_preds = self.regressor(features)
        return bbox_preds

# Instantiate the model
model = ObjectDetectionModel(backbone)


from torch.utils.data import DataLoader

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)


criterion = nn.SmoothL1Loss() # Also known as Huber loss - can try different loss functions later though
optimizer = torch.optim.Adam(model.regressor.parameters(), lr=1e-3)


if torch.backends.mps.is_available():
    device = torch.device('mps')
elif torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

model.to(device)

num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0
    for images, targets in train_loader:
        images = torch.stack(images).to(device)
        # Assuming each sample has one bounding box
        targets = torch.stack([t[0] for t in targets]).to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
    epoch_loss /= len(train_loader)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}")


The problem is that when I run this last cell - I get this error:

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[250], line 15
     13 model.train()
     14 epoch_loss = 0
---> 15 for images, targets in train_loader:
     16     images = torch.stack(images).to(device)
     17     # Assuming each sample has one bounding box

File /opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630, in _BaseDataLoaderIter.__next__(self)
    627 if self._sampler_iter is None:
    628     # TODO(https://github.com/pytorch/pytorch/issues/76750)
    629     self._reset()  # type: ignore[call-arg]
--> 630 data = self._next_data()
    631 self._num_yielded += 1
    632 if self._dataset_kind == _DatasetKind.Iterable and \
    633         self._IterableDataset_len_called is not None and \
    634         self._num_yielded > self._IterableDataset_len_called:

File /opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673, in _SingleProcessDataLoaderIter._next_data(self)
    671 def _next_data(self):
    672     index = self._next_index()  # may raise StopIteration
--> 673     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    674     if self._pin_memory:
    675         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)

File /opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55, in _MapDatasetFetcher.fetch(self, possibly_batched_index)
     53 else:
     54     data = self.dataset[possibly_batched_index]
---> 55 return self.collate_fn(data)

File /opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:317, in default_collate(batch)
    256 def default_collate(batch):
    257     r"""
    258     Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.
    259 
   (...)
    315         >>> default_collate(batch)  # Handle `CustomType` automatically
    316     """
--> 317     return collate(batch, collate_fn_map=default_collate_fn_map)

File /opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:174, in collate(batch, collate_fn_map)
    171 transposed = list(zip(*batch))  # It may be accessed twice, so we use a list.
    173 if isinstance(elem, tuple):
--> 174     return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
    175 else:
    176     try:

File /opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:142, in collate(batch, collate_fn_map)
    140 if collate_fn_map is not None:
    141     if elem_type in collate_fn_map:
--> 142         return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
    144     for collate_type in collate_fn_map:
    145         if isinstance(elem, collate_type):

File /opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:214, in collate_tensor_fn(batch, collate_fn_map)
    212     storage = elem._typed_storage()._new_shared(numel, device=elem.device)
    213     out = elem.new(storage).resize_(len(batch), *list(elem.size()))
--> 214 return torch.stack(batch, 0, out=out)

RuntimeError: stack expects each tensor to be equal size, but got [1, 4] at entry 0 and [7, 4] at entry 2




I am not sure how to fix it. 

Also here are some tips - make sure to make your code such that it works on cuda if cuda is availbe if not on mps if it is availbe and else on cpu

After you do 1.2 - do 1.3 next which is this:

Next, unfreeze the weights of your feature extraction network and ensure that the feature extraction backbone and the bounding box regression module are connected with gradients. Train both the back- bone and the bounding box prediction module together and observe if there is any improvement in model performance. Finally, report the Intersection over Union (IoU) for the model and visualize a few exam- ples of your bounding box predictions.